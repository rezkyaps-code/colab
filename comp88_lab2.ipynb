{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPZE2G81ZNke+yAgxt62jvr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comp0088/colab/blob/main/comp88_lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COMP0088 Lab Assignment 2\n",
        "\n"
      ],
      "metadata": {
        "id": "iZgl8O7b-fi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "\n",
        "In this week's session you are asked to implement the fitting of **linear models** for **regression** (using the normal equations) and **classification** (using gradient descent). You will also apply **basis expansion** to fit a **polynomial** using linear regression.\n",
        "\n",
        "Some examples of the sort of graphical outputs that should be generated once your code is implemented are given below. Plotting code is provided, so your plots should look pretty similar, although there may be quite a bit of random variation in the data.\n",
        "\n",
        "![example of completed plots](https://comp0088.github.io/assets/colab/week_2_small.jpg)\n",
        "\n",
        "Hopefully you are somewhat familiar with using Colab after last week's lab, but all the same points apply. In particular, remember that code edits do not automatically update to the notebook environment, you need to **run the cell** again to make them available.\n"
      ],
      "metadata": {
        "id": "N12ZHil1_ZVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up"
      ],
      "metadata": {
        "id": "hxzyJ3xeT4LB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usual, this notebook makes use of the NumPy library for numerical computing and the Matplotlib library for plotting, so we need to import them."
      ],
      "metadata": {
        "id": "4vHvSz5pReci"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL8UJ7lgLznk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# this is probably the default, but just in case\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also bring in the COMP0088 `utils` module from GitHub:"
      ],
      "metadata": {
        "id": "K1RLN5QATflG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load lab code and resources\n",
        "!git clone https://github.com/comp0088/shared.git comp0088\n",
        "\n",
        "# at the moment this is all we care about\n",
        "import comp0088.utils as utils"
      ],
      "metadata": {
        "id": "v3X7LDC5KAob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following functions for data generation are just copied from the solutions to last week's lab. Feel free to substitute your own versions! We'll use these to produce data to be used for learning this week."
      ],
      "metadata": {
        "id": "t1rjWDV14cGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_noisy_linear(num_samples, weights, sigma, limits, rng):\n",
        "    return utils.random_sample(lambda x: utils.affine(x, weights),\n",
        "                               len(weights) - 1,\n",
        "                               num_samples, limits, rng, sigma)\n",
        "\n",
        "def generate_linearly_separable(num_samples, weights, limits, rng):\n",
        "    def hyperplane_label(X, boundary):\n",
        "        y = utils.affine(X, boundary)\n",
        "        return (y > 0).astype(np.float64)\n",
        "\n",
        "    return utils.random_sample(lambda x: hyperplane_label(x, weights),\n",
        "                               count = len(weights) - 1,\n",
        "                               num_samples = num_samples,\n",
        "                               limits = limits,\n",
        "                               rng = rng)"
      ],
      "metadata": {
        "id": "07Nkw0QS4tho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finally, set up some items for use in later code\n",
        "shared_rng = numpy.random.default_rng()\n",
        "\n",
        "WEIGHTS = np.array([0.5, -0.4, 0.6])\n",
        "LIMITS = (-5, 5)"
      ],
      "metadata": {
        "id": "PfZQlfuELVwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Ridge regression\n",
        "\n",
        "Ridge regression is an extension of ordinary least squares (OLS) regression with a regularising penalty on the (squared) $L_2$ norm of the fitted weights. That is, given inputs $\\mathbf{X}$ and corresponding outputs $\\mathbf{y}$, we seek a vector $\\mathbf{w}^{\\star}$ such that:\n",
        "\n",
        "$$\n",
        "\\mathbf{w}^{\\star}  = \\text{argmin}_{\\mathbf{w}} \\left\\| \\mathbf{Xw - y} \\right\\|^2 + \\lambda \\left\\| \\mathbf{w} \\right\\|^2\n",
        "$$\n",
        "\n",
        "where $\\lambda$ is a hyperparameter that specifies the amount of regularisation. When $\\lambda = 0$ the problem reduces to OLS.\n",
        "\n",
        "Unlike many machine learning optimisations, ridge regression can be solved in closed form:\n",
        "\n",
        "$$\n",
        "\\mathbf{w}^{\\star} = (\\mathbf{X}^{\\mathsf{T}} \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}^{\\mathsf{T}} \\mathbf{y}\n",
        "$$\n",
        "\n",
        "Note that this solution requires computing a matrix inverse, which can be computationally expensive. So a numerical optimisation approach may still be preferable for larger problems.\n"
      ],
      "metadata": {
        "id": "FXpzvXtJAr4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Implement ridge regression in closed form\n",
        "\n",
        "Implement the body of the `ridge_closed` function in the cell below.\n",
        "\n",
        "The NumPy function [`linalg.inv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html) can be used to find the inverse of a matrix, though if you can rearrange the problem in suitable form for [`linalg.solve`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html) that is likely to be more efficient and numerically stable.\n",
        "\n",
        "Note that both functions will fail if the problem is underdetermined. The [pseudo-inverse](https://en.wikipedia.org/wiki/Moore–Penrose_inverse) calculated by [`linalg.pinv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.pinv.html) may be useful in that case, but you can assume it is not required here.\n",
        "\n",
        "The driver code below will plot fits to the same data using multiple regularisation weights. The sample data itself is randomly generated using a noisy linear model function like the one you wrote last week, so the data will differ each time you run. How do variations in the data affect the fits?"
      ],
      "metadata": {
        "id": "RGJxos1yA34M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DvgSCW9z6oXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ridge_closed ( X, y, l2=0 ):\n",
        "    \"\"\"\n",
        "    Implement L2-penalised least-squares (ridge) regression\n",
        "    using its closed form expression.\n",
        "\n",
        "    # Arguments\n",
        "        X: an array of sample data, where rows are samples\n",
        "           and columns are features (assume there are at least\n",
        "           as many samples as features). caller is responsible\n",
        "           for prepending x0=1 terms if required.\n",
        "        y: vector of measured (or simulated) labels for the samples,\n",
        "           must be same length as number of rows in X\n",
        "        l2: optional L2 regularisation weight. if zero (the default)\n",
        "           then this reduces to unregularised least squares\n",
        "\n",
        "    # Returns\n",
        "        w: the fitted vector of weights\n",
        "    \"\"\"\n",
        "    assert(len(X.shape)==2)\n",
        "    assert(X.shape[0]==len(y))\n",
        "\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "tbnDPv-S6pHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 1\n",
        "\n",
        "Execute the code cell below to run your ridge regression on some generated data and plot the results.\n",
        "\n",
        "Try playing with different values for `NUM_SAMPLES` and `SIGMA` and see how this affects the fit results.\n",
        "\n",
        "If you are feeling completist, have a look at the code for `plot_ridge_regression_1d`. Do you notice anything problematic about it? (This is not a question about software engineering or code style!)"
      ],
      "metadata": {
        "id": "NPwNjtpYh_F2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_ridge_regression_1d ( axes, X, y, weights, limits, l2s=[0] ):\n",
        "    \"\"\"\n",
        "    Perform least-squares fits to the provided (X, y) data\n",
        "    using the specified levels of L2 regularisation, and plot\n",
        "    the results.\n",
        "\n",
        "    # Arguments\n",
        "        axes: a Matplotlib Axes object into which to plot\n",
        "        X: an array of sample data, where rows are samples\n",
        "            and the single column is the input feature.\n",
        "        y: vector of output values corresponding to the\n",
        "            rows of X\n",
        "        weights: a weight vector of length 2, specifying\n",
        "            the true generating model, with a bias term\n",
        "            at index 0.\n",
        "        limits: a tuple (low, high) specifying the value\n",
        "            range of the feature dimension x1\n",
        "        l2s: a list (or vector/array) of numeric values\n",
        "            specifying amounts of L2 regularisation to use.\n",
        "    \"\"\"\n",
        "    assert(len(X.shape)==2)\n",
        "    assert(X.shape[1]==1)\n",
        "    assert(X.shape[0]==len(y))\n",
        "\n",
        "    # plot the data\n",
        "    axes.scatter(X[:,0], y, marker='x', color='grey')\n",
        "\n",
        "    # plot the true relationship\n",
        "    y0 = weights[0] + limits[0] * weights[1]\n",
        "    y1 = weights[0] + limits[1] * weights[1]\n",
        "\n",
        "    axes.plot(limits, (y0, y1), linestyle='dashed', color='red', label='Ground Truth')\n",
        "\n",
        "    # fit for specified regs and plot the results\n",
        "    X1 = utils.add_x0(X)\n",
        "\n",
        "    cmap = matplotlib.colormaps['jet']\n",
        "    for l2 in l2s:\n",
        "        w = ridge_closed(X1, y, l2)\n",
        "\n",
        "        y0 = w[0] + limits[0] * w[1]\n",
        "        y1 = w[0] + limits[1] * w[1]\n",
        "\n",
        "        axes.plot(limits, (y0, y1), linestyle='solid', color=cmap(l2/np.max(l2s)), label='$\\lambda=%.f$' % l2)\n",
        "\n",
        "    axes.set_xlim(limits[0], limits[1])\n",
        "    axes.set_ylim(limits[0], limits[1])\n",
        "    axes.set_xlabel('$x_1$')\n",
        "    axes.set_ylabel('$y$')\n",
        "\n",
        "    axes.legend(loc='upper left')\n",
        "\n",
        "    axes.set_title('Ridge Regression')\n",
        "\n",
        "NUM_SAMPLES = 50\n",
        "SIGMA = 1\n",
        "\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "axs = fig.subplots()\n",
        "\n",
        "X, y = generate_noisy_linear(NUM_SAMPLES, WEIGHTS, SIGMA, LIMITS, shared_rng)\n",
        "\n",
        "# note that here we are using the default l2=0, so this is unregularised OLS\n",
        "w = ridge_closed(utils.add_x0(X), y)\n",
        "\n",
        "print('true weights: %.2f, %.2f, %.2f' % (WEIGHTS[0], WEIGHTS[1], WEIGHTS[2]))\n",
        "\n",
        "if w is None:\n",
        "    print('regression not implemented')\n",
        "    utils.plot_unimplemented(axs, title='Ridge Regression')\n",
        "else:\n",
        "    print('regressed weights: %.2f, %.2f, %.2f' % (w[0], w[1], w[2]))\n",
        "    print('squared error: %.2g' % np.dot(WEIGHTS-w, WEIGHTS-w))\n",
        "\n",
        "    print('plotting regularised least squares')\n",
        "    X, y = generate_noisy_linear(NUM_SAMPLES, WEIGHTS[1:], SIGMA, LIMITS, shared_rng)\n",
        "    plot_ridge_regression_1d ( axs, X, y, WEIGHTS[1:], LIMITS, np.arange(5) * 25 )\n"
      ],
      "metadata": {
        "id": "m5XyX-uSFWON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Linear models with basis expansion\n",
        "\n",
        "Ridge regression and similar linear fitting approaches only require that the outputs are linear in the *parameters*. Fixed transformations of the inputs may be performed to fit *non-linear* functions of the *data*. The transformed data can be considered as a projection of the original samples into a new higher dimensional feature space.\n",
        "\n",
        "For example, a **monomial basis** comprises the *products* of the source features (including with themselves). For a 1D feature space, the products are just the powers of that single feature, ie: $x \\mapsto [ x^0, x^1, x^2, ..., x^k ]$ for some maximum degree $k$. A polynomial is just a weighted sum of monomial terms---that is, a vector in the monomial basis, which we can linearly fit.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FMWP8eI3Ba46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Map 1D feature vectors to a monomial basis\n",
        "\n",
        "Implement the `monomial_projection_1d` function below.\n"
      ],
      "metadata": {
        "id": "rpZPvjqBBmqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def monomial_projection_1d ( X, degree ):\n",
        "    \"\"\"\n",
        "    Map 1d data to an expanded basis of monomials\n",
        "    up to the given degree.\n",
        "\n",
        "    # Arguments\n",
        "        X: an array of sample data, where rows are samples\n",
        "            and the single column is the input feature.\n",
        "        degree: maximum degree of the monomial terms\n",
        "\n",
        "    # Returns\n",
        "        Xm: an array of the transformed data, with the\n",
        "            same number of rows (samples) as X, and\n",
        "            with degree+1 columns (features):\n",
        "            1, x, x**2, x**3, ..., x**degree\n",
        "    \"\"\"\n",
        "    assert(len(X.shape)==2)\n",
        "    assert(X.shape[1]==1)\n",
        "\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "2rTVOKdyLFMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Generate noisy 1D polynomial data\n",
        "\n",
        "Provide code to implement the function `generate_noisy_poly_1d` below.\n",
        "\n",
        "This function is obviously quite similar to `generate_noisy_linear` from week 1. Note that `weights` includes a bias term, so the monomial degree will be 1 less than its length."
      ],
      "metadata": {
        "id": "gb13HFsdB1Pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_noisy_poly_1d ( num_samples, weights, sigma, limits, rng ):\n",
        "    \"\"\"\n",
        "    Draw samples from a 1D polynomial model with additive\n",
        "    Gaussian noise.\n",
        "\n",
        "    # Arguments\n",
        "        num_samples: number of samples to generate\n",
        "            (ie, the number of rows in the returned X\n",
        "            and the length of the returned y)\n",
        "        weights: vector of the polynomial coefficients\n",
        "            (including a bias term at index 0)\n",
        "        sigma: standard deviation of the additive noise\n",
        "        limits: a tuple (low, high) specifying the value\n",
        "            range for the single input dimension x1\n",
        "        rng: an instance of numpy.random.Generator\n",
        "            from which to draw random numbers\n",
        "\n",
        "    # Returns\n",
        "        X: a matrix of sample inputs, where\n",
        "            the samples are the rows and the\n",
        "            single column is the 1D feature x1\n",
        "            ie, its size should be:\n",
        "              num_samples x 1\n",
        "        y: a vector of num_samples output values\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "    return None, None"
      ],
      "metadata": {
        "id": "SEYLZNSzLFok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Implement linear fitting of a 1D polynomial\n",
        "\n",
        "Complete the implementation of the `fit_poly_1d` function in the cell below. Use your `monomial_projection_1d` function to transform the data and your `ridge_closed` implementation from Task 1 to perform the fitting."
      ],
      "metadata": {
        "id": "O_6LsBT0RCNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_poly_1d ( X, y, degree, l2=0 ):\n",
        "    \"\"\"\n",
        "    Fit a polynomial of the given degree to 1D sample data.\n",
        "\n",
        "    # Arguments\n",
        "        X: an array of sample data, where rows are samples\n",
        "            and the single column is the input feature.\n",
        "        y: vector of output values corresponding to the inputs,\n",
        "           must be same length as number of rows in X\n",
        "        degree: degree of the polynomial\n",
        "        l2: optional L2 regularisation weight\n",
        "\n",
        "    # Returns\n",
        "        w: the fitted polynomial coefficients\n",
        "    \"\"\"\n",
        "    assert(len(X.shape)==2)\n",
        "    assert(X.shape[1]==1)\n",
        "    assert(X.shape[0]==len(y))\n",
        "\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "wbG26t1eRWk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 2\n",
        "\n",
        "Execute the code cell below to run the function you wrote above and generate a plot.\n",
        "\n",
        "As in Task 1, try playing with different values for `NUM_SAMPLES` and `SIGMA` and see how this affects the fit results."
      ],
      "metadata": {
        "id": "7cOF1Dyfig7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_poly_fit_1d ( axes, X, y, weights, limits, degrees, l2=0 ):\n",
        "    \"\"\"\n",
        "    Fit polynomials of different degrees to the supplied\n",
        "    data, and plot the results.\n",
        "\n",
        "    # Arguments\n",
        "        axes: a Matplotlib Axes object into which to plot\n",
        "        X: an array of sample data, where rows are samples\n",
        "            and the single column is the input feature.\n",
        "        y: vector of output values corresponding to the inputs,\n",
        "           must be same length as number of rows in X\n",
        "        weights: the true polynomial coefficients from which\n",
        "           the data was generated\n",
        "        limits: a tuple (low, high) specifying the value\n",
        "            range of the feature dimension x1\n",
        "        degrees: a list of integer values specifying degrees\n",
        "            of polynomial to fit\n",
        "        l2: the amount of l2 regularisation to apply\n",
        "\n",
        "    # Returns\n",
        "        None\n",
        "    \"\"\"\n",
        "    assert(len(X.shape)==2)\n",
        "    assert(X.shape[1]==1)\n",
        "    assert(X.shape[0]==len(y))\n",
        "\n",
        "    axes.scatter(X, y, color='grey', marker='x')\n",
        "\n",
        "    print(f'true weights: {weights}')\n",
        "    ground_x, ground_y = utils.grid_sample(lambda x: utils.affine(monomial_projection_1d(x, len(weights)-1), weights),\n",
        "                                           1,\n",
        "                                           num_divisions=50, limits=limits)\n",
        "\n",
        "    axes.plot(ground_x, ground_y, color='red', linestyle='dashed', label='Ground Truth')\n",
        "\n",
        "    cmap = matplotlib.colormaps['jet']\n",
        "    n = 0\n",
        "    for deg in degrees:\n",
        "        w = fit_poly_1d(X, y, deg, l2)\n",
        "\n",
        "        if w is None:\n",
        "            print('Polynomial fitting not implemented')\n",
        "            break\n",
        "\n",
        "        print(f'fit {deg} weights: {w}')\n",
        "        fit_x, fit_y = utils.grid_sample(lambda x: utils.affine(monomial_projection_1d(x, len(w)-1), w),\n",
        "                                         1,\n",
        "                                         num_divisions=50, limits=limits)\n",
        "        axes.plot(fit_x, fit_y, linestyle='solid', color=cmap(n/len(degrees)), label=f'Degree {deg} Fit')\n",
        "        n += 1\n",
        "\n",
        "    axes.set_xlim(limits[0], limits[1])\n",
        "    axes.set_xlabel('$x_1$')\n",
        "    axes.set_ylabel('$y$')\n",
        "\n",
        "    axes.legend(loc='upper right')\n",
        "\n",
        "    axes.set_title('Polynomial Fitting')\n",
        "\n",
        "NUM_SAMPLES = 50\n",
        "SIGMA = 3\n",
        "\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "ax = fig.subplots()\n",
        "\n",
        "X, y = generate_noisy_poly_1d ( NUM_SAMPLES, WEIGHTS, SIGMA, LIMITS, shared_rng )\n",
        "if X is None:\n",
        "    print('poly generation not implemented')\n",
        "    utils.plot_unimplemented(ax, title='Polynomial Fitting')\n",
        "else:\n",
        "    plot_poly_fit_1d ( ax, X, y, WEIGHTS, LIMITS, [1, 2, 3, 4], 0 )"
      ],
      "metadata": {
        "id": "o-zoeOPlLG15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Gradient descent\n",
        "\n",
        "**Gradient descent** is an iterative numerical method for solving optimisation problems where an analytic solution is unavailable or impractical. It is the basis for a family of closely-related methods that are very widely applied in ML and deep learning.\n",
        "\n",
        "The basic procedure is shown in pseudocode form below; we might facetiously summarise it as \"walk downhill\".\n",
        "\n",
        "```\n",
        "function GradientDescent( f, z0, alpha )\n",
        "    f: the function to be minimised\n",
        "    z0: an initial value for the function parameter vector z\n",
        "    alpha: some small step size, known as the learning rate\n",
        "\n",
        "    z = z0\n",
        "    repeat:\n",
        "      z -= alpha * Gradient(f, z)\n",
        "    until (some stopping criterion reached)\n",
        "    return z\n",
        "```\n",
        "\n",
        "Note the following:\n",
        "\n",
        "* The algorithm requires the gradient of the function with respect to the parameters, `Gradient(f, z)`, to be calculable, so `f` must be (more or less) continuous and differentiable. (There can be scope for fudging over known undefined points -- see the lasso regularisation task in Further Exploration.)\n",
        "* Only local information is used at each step, so the process can get stuck in local minima if `f` is not convex.\n",
        "* There is a hyperparameter `alpha` that must be chosen somehow, which may have a significant effect on how long the process takes to converge (or whether it does at all).\n",
        "* When the gradient is shallow, convergence might be very slow.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IhLI-HSFUjj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Implement a generic gradient descent optimiser\n",
        "\n",
        "Provide an implementation of the `gradient_descent` function in the cell below. It receives function objects for both the function to be minimised and its gradient and should repeat the process until any of the convergence criteria are reached.\n",
        "\n",
        "Note that the returned values are lists documenting the *history* of the optimisation.\n"
      ],
      "metadata": {
        "id": "Xxl22B-kXqRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent ( z, loss_func, grad_func, lr=0.01,\n",
        "                       loss_stop=1e-4, z_stop=1e-4, max_iter=100 ):\n",
        "    \"\"\"\n",
        "    Generic batch gradient descent optimisation.\n",
        "    Iteratively updates z by subtracting lr * grad\n",
        "    until one or more stopping criteria are met.\n",
        "\n",
        "    # Arguments\n",
        "        z: initial value(s) of the optimisation var(s).\n",
        "            can be a scalar if optimising a univariate\n",
        "            function, otherwise a single numpy array\n",
        "        loss_func: function of z that we seek to minimise,\n",
        "            should return a scalar value\n",
        "        grad_func: function calculating the gradient of\n",
        "            loss_func at z. for vector z, this should return\n",
        "            a vector of the same length containing the\n",
        "            partial derivatives\n",
        "        lr: learning rate, ie fraction of the gradient by\n",
        "            which to update z each iteration\n",
        "        loss_stop: stop iterating if the loss changes\n",
        "            by less than this (absolute)\n",
        "        z_stop: stop iterating if z changes by less than\n",
        "            this (L2 norm)\n",
        "        max_iter: stop iterating after iterating this\n",
        "            many times\n",
        "\n",
        "    # Returns\n",
        "        zs: a list of the z values at each iteration\n",
        "        losses: a list of the losses at each iteration\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "    return None, None"
      ],
      "metadata": {
        "id": "yHp-QfcBX5ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 3\n",
        "\n",
        "Execute the cell below to test your implementation.\n",
        "\n",
        "Although our ultimate goal for gradient descent is to fit the parameters of machine learning models from potentially noisy data, the algorithm can be used to minimise any reasonably well-behaved function for which the gradient can be calculated. The test code applies it to a simple quadratic function,\n",
        "\n",
        "$$\n",
        "f(x) = x^2 - 2x + 1\n",
        "$$\n",
        "\n",
        "for which the gradient is trivially derived,\n",
        "\n",
        "$$\n",
        "f^\\prime(x) = 2x - 2\n",
        "$$\n",
        "\n",
        "and we can see that the function reaches a minimum of 0 at $x=1$. Check that your gradient descent implementation manages to adequately estimate this result."
      ],
      "metadata": {
        "id": "SG9VU7mwYtzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xx, ll = gradient_descent ( 10,\n",
        "                            lambda x: x * x - 2 * x + 1,\n",
        "                            lambda x: 2 * x - 2,\n",
        "                            lr=0.1,\n",
        "                            loss_stop=1e-6,\n",
        "                            z_stop=1e-6 )\n",
        "\n",
        "print(f'Minimum value found after {len(xx)} iterations: {xx[-1]:.4f}')\n",
        "print(f'Final loss value: {ll[-1]}')"
      ],
      "metadata": {
        "id": "B9gZpHH3YuXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4: Logistic regression\n",
        "\n",
        "Logistic regression is an adaptation of linear regression for binary classification. It fits a linear decision boundary between two classes using a **sigmoid** transformation of the output value that maps it from $(-\\infty, +\\infty)$ to $(0, 1)$:\n",
        "\n",
        "$$\n",
        "\\hat{\\mathbf{y}} = \\sigma(\\mathbf{Xw})\n",
        "$$\n",
        "\n",
        "where $\\sigma$ is the **logistic function**:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "The predictions $\\hat{\\mathbf{y}}$ are evaluated against the true labels $\\mathbf{y}$ using a **binary cross-entropy** loss:\n",
        "\n",
        "$$\n",
        "L(\\mathbf{y}, \\hat{\\mathbf{y}}) = -\\frac{1}{n} \\; \\Bigg[ \\; \\mathbf{y} \\cdot \\log(\\hat{\\mathbf{y}}) + (1 - \\mathbf{y}) \\cdot \\log(1 - \\hat{\\mathbf{y}}) \\; \\Bigg]\n",
        "$$\n",
        "\n",
        "(The loss is expressed here using dot product notation rather than the more common summation, but the two are equivalent and this representation lends itself to a concise implementation in NumPy.)\n",
        "\n",
        "Unlike for ridge regression, there is no closed form expression for finding the $\\mathbf{w}$ that minimises this loss, and we must instead optimise numerically. Luckily, the loss function is differentiable and convex, so gradient descent is a reasonable approach. (Other algorithms are also available that may converge faster, but we won't worry about that here.)\n",
        "\n",
        "As shown in the lectures, the gradient of the loss with respect to $\\mathbf{w}$ is:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\mathbf{w}}L = \\mathbf{X}^{\\mathsf{T}} (\\hat{\\mathbf{y}} - \\mathbf{y})\n",
        "$$"
      ],
      "metadata": {
        "id": "swxeclj-B54s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Implement logistic regression\n",
        "\n",
        "Provide an implementation body for the `logistic_regression` function defined in the cell below.\n",
        "\n",
        "Use the `gradient_descent` function from Task 3 to do the optimisation.\n",
        "\n",
        "You will need to provide functions for the loss and gradient that take only the weights vector as their argument. (Remember that the training data is constant as far as parameter fitting is concerned.)\n",
        "\n",
        "One thing to note when implementing this is that errors are likely to arise from trying to take the logarithm of zero. Strictly, the logistic function is never quite 0 or 1, but it gets *really* close. You will probably find it helpful to add a small offset value to $\\hat{\\mathbf{y}}$ and $1 - \\hat{\\mathbf{y}}$ when calculating the loss.\n",
        "\n"
      ],
      "metadata": {
        "id": "KXLDZUltCAFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression ( X, y, w0=None, lr=0.05,\n",
        "                          loss_stop=1e-4, weight_stop=1e-4, max_iter=100 ):\n",
        "    \"\"\"\n",
        "    Fit a logistic regression classifier to data.\n",
        "\n",
        "    # Arguments\n",
        "        X: an array of sample data, where rows are samples\n",
        "           and columns are features. caller is responsible\n",
        "           for prepending x0=1 terms if required.\n",
        "        y: vector of binary class labels for the samples,\n",
        "           must be same length as number of rows in X\n",
        "        w0: starting value of the weights, if omitted\n",
        "           then all zeros are used\n",
        "        lr: learning rate, ie fraction of gradients by\n",
        "           which to update weights at each iteration\n",
        "        loss_stop: stop iterating if the loss changes\n",
        "            by less than this (absolute)\n",
        "        weight_stop: stop iterating if weights change by less\n",
        "            than this (L2 norm)\n",
        "        max_iter: stop iterating after iterating this\n",
        "            many times\n",
        "\n",
        "    # Returns\n",
        "        ws: a list of fitted weights at each iteration\n",
        "        losses: a list of the loss values at each iteration\n",
        "    \"\"\"\n",
        "    assert(len(X.shape)==2)\n",
        "    assert(X.shape[0]==len(y))\n",
        "\n",
        "    # TODO: implement this\n",
        "    return None, None\n"
      ],
      "metadata": {
        "id": "SA_hr24lNqOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 4\n",
        "\n",
        "Execute the code cell below to run the function you wrote in the previous cell and produce plots showing the loss history and eventual solution.\n"
      ],
      "metadata": {
        "id": "7DBIxl_ZitVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_logistic_regression_2d ( axs, X, y, weights, limits ):\n",
        "    \"\"\"\n",
        "    Fit a 2D logistic regression classifier and plot the results.\n",
        "    Note that there are two separate plots produced here.\n",
        "    The first (in axs[0]) is an optimisation history, showing how the\n",
        "    loss decreases via gradient descent. The second (in axs[1]) is\n",
        "    the regression itself, showing data points and fit results.\n",
        "\n",
        "    # Arguments\n",
        "        axs: an array of 2 Matplotlib Axes objects into which\n",
        "           to plot.\n",
        "        X: an array of sample data, where rows are samples\n",
        "           and columns are features, including x0=1 terms.\n",
        "        y: vector of binary class labels for the samples,\n",
        "           must be same length as number of rows in X\n",
        "        weights: weights defining the true decision boundary\n",
        "           with which the data was generated\n",
        "        limits: a tuple (low, high) specifying the value\n",
        "            range of both feature dimensions\n",
        "\n",
        "    # Returns\n",
        "        None\n",
        "    \"\"\"\n",
        "    assert(len(X.shape)==2)\n",
        "    assert(X.shape[1]==3)\n",
        "    assert(X.shape[0]==len(y))\n",
        "    assert(len(weights)==3)\n",
        "\n",
        "    ww, ll = logistic_regression(X, y)\n",
        "    if ww is None:\n",
        "        utils.plot_unimplemented(axs[0], title='Logistic Regression Gradient Descent')\n",
        "        utils.plot_unimplemented(axs[1], title='Logistic Regression Results')\n",
        "        return\n",
        "\n",
        "    print('Number of iterations: %i' % len(ll))\n",
        "    axs[0].plot(ll)\n",
        "    axs[0].set_title('Logistic Regression Gradient Descent')\n",
        "    axs[0].set_xlabel('Iteration')\n",
        "    axs[0].set_ylabel('Logistic Loss')\n",
        "\n",
        "    Xm, ym = utils.grid_sample(lambda x: 1/(1 + np.exp(-utils.affine(x, ww[-1]))), 2, 100, limits)\n",
        "    axs[1].imshow(ym.T, cmap='coolwarm', origin='lower', extent=(limits[0], limits[1], limits[0], limits[1]), alpha=0.5)\n",
        "    axs[1].contour(ym.T, levels=[.5], origin='lower', extent=(limits[0], limits[1], limits[0], limits[1]))\n",
        "\n",
        "    y0 = -(weights[0] + limits[0] * weights[1]) / weights[2]\n",
        "    y1 = -(weights[0] + limits[1] * weights[1]) / weights[2]\n",
        "\n",
        "    axs[1].plot(limits, (y0, y1), linestyle='dashed', color='red', marker='')\n",
        "\n",
        "    axs[1].plot(X[y==0,1], X[y==0,2], linestyle='', color='orange', marker='v', label='Class 0')\n",
        "    axs[1].plot(X[y==1,1], X[y==1,2], linestyle='', color='darkorchid', marker='o', label='Class 1')\n",
        "\n",
        "    axs[1].set_xlabel('$x_1$')\n",
        "    axs[1].set_ylabel('$x_2$')\n",
        "\n",
        "    axs[1].legend(loc='upper left', framealpha=1)\n",
        "\n",
        "    axs[1].set_title('Logistic Regression Results')\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(9, 4.5))\n",
        "axs = fig.subplots(ncols=2)\n",
        "\n",
        "NUM_SAMPLES = 50\n",
        "\n",
        "X, y = generate_linearly_separable(NUM_SAMPLES, WEIGHTS, LIMITS, shared_rng)\n",
        "X0 = utils.add_x0(X)\n",
        "plot_logistic_regression_2d(axs, X0, y, WEIGHTS, LIMITS)\n",
        "\n",
        "fig.tight_layout(pad=1)"
      ],
      "metadata": {
        "id": "T-C7I1zBNr4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further exploration\n",
        "\n",
        "If you have exhausted the previous exercises, you might find it interesting to try out one or more of the following challenges. Doing so is entirely optional, but may provide some additional perspective that could be useful in the weeks ahead."
      ],
      "metadata": {
        "id": "ENAVpErdCTaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement lasso regression using gradient descent\n",
        "\n",
        "The loss function for **lasso** ($L_1$ regularised least squares) looks like this:\n",
        "\n",
        "$$\n",
        "L(\\mathbf{X, y, w}) = \\|\\mathbf{Xw - y}\\|^2 + \\lambda \\| \\mathbf{w} \\|_1\n",
        "$$\n",
        "\n",
        "The $L_1$ norm has a corner at zero, so this function is not strictly differentiable. However, it is easy to define a **subgradient** for the norm at this point -- this could be any value in the interval $[-1, 1]$, but since this really *is* its minimum point the obvious choice is 0.\n",
        "\n",
        "Armed with this subgradient, you should be able to implement the lasso loss and gradient functions and fit a lasso model by gradient descent. How do the results of such a fit compare with those from ridge regression?\n"
      ],
      "metadata": {
        "id": "HqJRn5FmCXNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement multinomial logistic regression using gradient descent\n",
        "\n",
        "As discussed in the lectures, the binary classification model of logistic regression can be adapted for multiclass problems using **softmax** squashing and a **categorical cross-entropy** loss. The model becomes:\n",
        "\n",
        "$$\n",
        "\\hat{\\mathbf{Y}} = \\mathbf{\\varsigma}(\\mathbf{XW})\n",
        "$$\n",
        "\n",
        "where $\\mathbf{\\varsigma}$ is the softmax function:\n",
        "\n",
        "$$\n",
        "\\mathbf{\\varsigma}(\\mathbf{z}) = \\frac{e^{\\mathbf{z}}}{\\|e^{\\mathbf{z}}\\|_1}\n",
        "$$\n",
        "\n",
        "with exponentiation performed elementwise. The loss is:\n",
        "\n",
        "$$\n",
        "L(\\mathbf{Y, \\hat{Y}}) = -\\frac{1}{n} \\sum_i^n \\mathbf{y}_i \\cdot \\log(\\hat{\\mathbf{y}}_i) = \\frac{1}{n} \\Big\\| \\mathbf{Y}^\\mathsf{T}\\log(\\hat{\\mathbf{Y}}) \\Big\\|_1\n",
        "$$\n",
        "\n",
        "where the logs are similarly taken elementwise. The gradient is:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\mathbf{W}}L = \\mathbf{X}^{\\mathsf{T}} (\\hat{\\mathbf{Y}} - \\mathbf{Y})\n",
        "$$\n",
        "\n",
        "Fitting this via gradient descent is similar to binary logistic regression, but you will need to manage mapping ground truth values into one-hot vectors and prediction vectors back to class labels. And remember that $\\mathbf{Y}$, $\\hat{\\mathbf{Y}}$ and $\\mathbf{W}$ are all matrices.\n",
        "\n",
        "Multinomial logistic regression can be applied to data with two classes, so you could test your implementation with synthetic data from the binary `generate_linearly_separable` function. But you'll probably want to try it with more than two classes. One good source of test data is the `sklearn.datasets` package from `scikit-learn`, which should already be installed in your Colab environment and ready to import. For example, have a look at the [`load_iris`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) function for Fisher's classic [Iris](https://en.wikipedia.org/wiki/Iris_flower_data_set) dataset.\n"
      ],
      "metadata": {
        "id": "5rf1J7K4L10r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement stochastic/mini-batch gradient descent\n",
        "\n",
        "In Task 3 you implemented *batch* gradient descent, where the loss gradient is calculated with respect to the whole training set before updating the parameters. In *mini-batch* gradient descent, updates are applied more frequently using gradients estimated using only a fraction of the training set. If that fraction is just a single sample at a time, then it is known as *stochastic* gradient descent. (In practice this distinction is quite often glossed over and the term \"stochastic gradient descent\" applied for any mini-batch size.)\n",
        "\n",
        "Try implementing mini-batch gradient descent with a configurable batch size and see how it behaves in comparison to the full batch version. (You will need to adapt the interfaces both for the optimiser and the loss functions to accept the training data.) Plot the loss history for all the mini-batch updates -- is it noticeably different to that for batch gradient descent? Does the batch size affect this?\n"
      ],
      "metadata": {
        "id": "m-usBkyvOAP9"
      }
    }
  ]
}