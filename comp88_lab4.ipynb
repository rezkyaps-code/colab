{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPhc5glFgD/2ERl9T8mIFrZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comp0088/colab/blob/main/comp88_lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COMP0088 Lab Assignment 4\n",
        "\n"
      ],
      "metadata": {
        "id": "iZgl8O7b-fi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In contrast to labs 2 & 3, this week's exercises are more concerned with how the learning algorithms behave than with the nuts and bolts of implementation. If you are especially enthusiastic there are [Further exploration]() tasks on implementing support vector machines, but otherwise we will just use a standard implementation from `scikit-learn`. Instead, you are invited to generate data that will probe the model behaviour.\n",
        "\n",
        "Examples of the kinds of plots that will be produced by your finished code are shown below.\n",
        "\n",
        "![example of completed plots](https://comp0088.github.io/assets/colab/week_4_small.jpg)\n"
      ],
      "metadata": {
        "id": "N12ZHil1_ZVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up"
      ],
      "metadata": {
        "id": "hxzyJ3xeT4LB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usual, this notebook makes use of the NumPy library for numerical computing and the Matplotlib library for plotting, so we need to import them. And as mentioned above, we will use the [scikit-learn](https://pandas.pydata.org/docs/user_guide/index.html) library for our support vector machines."
      ],
      "metadata": {
        "id": "4vHvSz5pReci"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL8UJ7lgLznk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.random\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# this is probably the default, but just in case\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also fetch the shared COMP0088 lab code from the module GitHub:"
      ],
      "metadata": {
        "id": "K1RLN5QATflG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load lab code and resources\n",
        "!git clone https://github.com/comp0088/shared.git comp0088\n",
        "\n",
        "# at the moment this is all we care about\n",
        "import comp0088.utils as utils"
      ],
      "metadata": {
        "id": "v3X7LDC5KAob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function is an SVM-specific wrapper for `utils.plot_classification_map` that adds markers for the support vectors."
      ],
      "metadata": {
        "id": "Xbo37TyHJNXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_svm_map ( axes, svm, X, y, resolution, title ):\n",
        "    \"\"\"\n",
        "    Utility to plot the classification map of an SVM\n",
        "    highlighting the support vectors.\n",
        "    \"\"\"\n",
        "    utils.plot_classification_map(axes, lambda z: svm.predict(z), X, y,\n",
        "                                  resolution=resolution, title=title, legend_loc=None)\n",
        "    axes.scatter(X[svm.support_,0], X[svm.support_,1],\n",
        "                 facecolors='none', edgecolors='k', s=140, label='SV')\n",
        "    axes.legend(loc='upper left')"
      ],
      "metadata": {
        "id": "MGlRvz1cJJgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up some items for later use."
      ],
      "metadata": {
        "id": "t1rjWDV14cGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# finally, set up some items for use in later code\n",
        "shared_rng = numpy.random.default_rng()\n",
        "\n",
        "LIMITS=(-5, 5)"
      ],
      "metadata": {
        "id": "PfZQlfuELVwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Linear SVM\n",
        "\n",
        "As discussed in the lectures, a linear SVM fits a decision boundary to maximise the **geometric margin** between that boundary and the worst case (i.e., nearest) training samples in each class. For a **hard margin** classifier, the classes must be **linearly separable**, whereas a **soft margin** SVM can allow misclassifications to an extent governed by a **cost** hyperparameter, $C$.\n",
        "\n",
        "The geometric margin for a sample $(\\mathbf{x}, y)$ is defined as:\n",
        "\n",
        "\\begin{equation}\n",
        "M_g = \\frac{y (\\mathbf{x}\\cdot\\mathbf{w} + b)}{\\|\\mathbf{w}\\|}\n",
        "\\end{equation}\n",
        "\n",
        "where $\\mathbf{w}$ and $b$ are the weights and bias defining the boundary, and $y$ uses the $\\{-1, +1\\}$ labelling convention (like Adaboost last week). The geometric margin for a whole dataset is the minimum of the geometric margins for all samples in the set.\n"
      ],
      "metadata": {
        "id": "FXpzvXtJAr4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Generate more realistic binary classification data\n",
        "\n",
        "In the exercises for week 1 you were asked to generate linearly separable data, but the generating model was quite unrealistic — most data is unlikely to exhibit such a tight and consistent boundary, with neither errors nor any appreciable margin between the classes. Such data doesn't provide much intuition for SVM behaviour. Here you should come up with something more plausible.\n",
        "\n",
        "Implement the body of the `generate_margined_binary_data` function in the cell below to provide a more realistic source of linearly separable data for which there actually is some separation between the classes.\n",
        "\n",
        "You are free to devise any generating model you like as long as it produces some non-negligible margin between the classes. If you have the time and inclination, it might be worth trying out more than one formulation. If you are short on inspiration, feel free to ask the TAs for suggestions.\n",
        "\n",
        "Note that the returned label vector should continue to use the same $\\{0, 1\\}$ labelling convention as in previous weeks."
      ],
      "metadata": {
        "id": "RGJxos1yA34M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_margined_binary_data ( num_samples, count, limits, rng ):\n",
        "    \"\"\"\n",
        "    Draw random samples from a linearly-separable binary model\n",
        "    with some non-negligible margin between classes. (The exact\n",
        "    form of the model is up to you.)\n",
        "\n",
        "    # Arguments\n",
        "        num_samples: number of samples to generate\n",
        "            (ie, the number of rows in the returned X\n",
        "            and the length of the returned y)\n",
        "        count: the number of feature dimensions\n",
        "        limits: a tuple (low, high) specifying the value\n",
        "            range of all the features x_i\n",
        "        rng: an instance of numpy.random.Generator\n",
        "            from which to draw random numbers\n",
        "\n",
        "    # Returns\n",
        "        X: a matrix of sample vectors, where\n",
        "            the samples are the rows and the\n",
        "            features are the columns\n",
        "            ie, its size should be:\n",
        "              num_samples x count\n",
        "        y: a vector of num_samples binary labels\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "    return None, None"
      ],
      "metadata": {
        "id": "tbnDPv-S6pHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Calculate the geometric margin of a linear decision boundary\n",
        "\n",
        "Implement the `geometric_margin` function in the cell below.\n",
        "\n",
        "Note that the supplied label vector `y` will use the $\\{0, 1\\}$ labelling convention, not $\\{-1, +1\\}$. You will need to account for this in your calculations."
      ],
      "metadata": {
        "id": "diDAHi3gF1uE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def geometric_margin ( X, y, weights, bias ):\n",
        "    \"\"\"\n",
        "    Calculate the geometric margin for a given\n",
        "    dataset and linear decision boundary. May be\n",
        "    negative if any of the samples are\n",
        "    misclassified.\n",
        "\n",
        "    # Arguments\n",
        "        X: an array of sample data, where rows are samples\n",
        "           and columns are features.\n",
        "        y: vector of ground truth labels for the samples,\n",
        "           must be same length as number of rows in X\n",
        "        weights: a vector of weights defining the direction\n",
        "           of the decision boundary, must be the same\n",
        "           length as the number of features\n",
        "        bias: scalar intercept value specifying the position\n",
        "           of the boundary\n",
        "\n",
        "    # Returns:\n",
        "        g: the geometric margin -- ie, the minimum distance\n",
        "           of any of the samples on the correct side of the\n",
        "           boundary (or the negative greatest distance on the\n",
        "           wrong side)\n",
        "    \"\"\"\n",
        "    assert(X.shape[0] == len(y))\n",
        "    assert(X.shape[1] == len(weights))\n",
        "\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "ZeOh5-PzF3RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 1\n",
        "\n",
        "Execute the code cell below to generate data, fit linear and RBF support vector machines to it, plot the results and estimate the geometric margin.\n",
        "\n",
        "As in last week's lab, feel free to try out different values of `NUM_SAMPLES` and `RESOLUTION`, and also play with the cost parameter `COST` and the kernel bandwidth `GAMMA`."
      ],
      "metadata": {
        "id": "NPwNjtpYh_F2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SAMPLES = 50\n",
        "RESOLUTION = 20\n",
        "COST = 1\n",
        "GAMMA = 0.5\n",
        "\n",
        "fig = plt.figure(figsize=(9, 5))\n",
        "axs = fig.subplots(ncols=2)\n",
        "\n",
        "# generate some data\n",
        "X_lin, y_lin = generate_margined_binary_data ( NUM_SAMPLES, 2, LIMITS, shared_rng )\n",
        "\n",
        "if X_lin is None:\n",
        "  print('generate_margined_binary_data not implemented')\n",
        "  utils.plot_unimplemented(axs[0], title='Linear Data, Linear SVM')\n",
        "  utils.plot_unimplemented(axs[1], title='Linear Data, RBF SVM')\n",
        "else:\n",
        "  svm_lin = SVC(kernel='linear', C=COST)\n",
        "  svm_lin.fit(X_lin, y_lin)\n",
        "\n",
        "  plot_svm_map ( axs[0], svm_lin, X_lin, y_lin, RESOLUTION, title='Linear Data, Linear SVM' )\n",
        "\n",
        "  svm_rbf = SVC(kernel='rbf', C=COST, gamma=GAMMA)\n",
        "  svm_rbf.fit(X_lin, y_lin)\n",
        "\n",
        "  plot_svm_map ( axs[1], svm_rbf, X_lin, y_lin, RESOLUTION, title='Linear Data, RBF SVM' )\n",
        "\n",
        "  # calculate the geometric margin of the linear SVM\n",
        "  marg_svm = geometric_margin(X_lin, y_lin, svm_lin.coef_[0,:], svm_lin.intercept_[0])\n",
        "\n",
        "  if marg_svm is None:\n",
        "    print('geometric_margin not implemented')\n",
        "  else:\n",
        "    print(f'Linear SVM geometric margin: {marg_svm:.3f}')"
      ],
      "metadata": {
        "id": "m5XyX-uSFWON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Perceptrons\n",
        "\n",
        "The perceptron is a simple single-layer linear model for  binary classification. Its decision function is exactly the linear model boundary decision function described in Task 2 of Lab 1:\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{y} = \\begin{cases}\n",
        "\t1 & \\text{if $\\mathbf{w}\\cdot\\mathbf{x} \\ge 0$}\\\\\n",
        "\t0 & \\text{otherwise}\n",
        "     \\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "The model is trained iteratively, one training sample at a time, updating the weights at each step as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{w} \\gets \\mathbf{w} + \\alpha (y - \\hat{y}) \\mathbf{x}\n",
        "\\end{equation}\n",
        "\n",
        "where $\\alpha$ is the learning rate. Training continues until there are no errors (and hence no updates) for the whole training set. Noting that $\\nabla_{\\mathbf{w}}(\\mathbf{w \\cdot x}) = \\mathbf{x}$ and $(y - \\hat{y})$ is the direction of any error, we can see that this update rule is effectively stochastic gradient descent on the **margin** of misclassified samples.\n",
        "\n",
        "The perceptron is guaranteed to converge for linearly separable data, though time to converge depends on the margin. The boundary may be non-optimal and depends on the order in which samples are presented. It is guaranteed *never* to converge when the data are not linearly separable — there will always be an error somewhere.\n"
      ],
      "metadata": {
        "id": "FMWP8eI3Ba46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Learn a decision boundary with the perceptron algorithm\n",
        "\n",
        "Implement the `perceptron_train` function in the code cell below.\n",
        "\n",
        "As mentioned in the docstring, you should return the final weights vector even if the algorithm fails to converge.\n",
        "\n",
        "Input labels will be $\\{0, 1\\}$.\n"
      ],
      "metadata": {
        "id": "rpZPvjqBBmqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perceptron_train ( X, y, alpha=1, max_epochs=50, include_bias=True ):\n",
        "    \"\"\"\n",
        "    Learn a linear decision boundary using the\n",
        "    perceptron algorithm.\n",
        "\n",
        "    # Arguments\n",
        "        X: an array of sample data, where rows are samples\n",
        "            and columns are features.\n",
        "        y: vector of ground truth labels for the samples,\n",
        "            must be same length as number of rows in X\n",
        "        alpha: learning rate, ie how much to adjust the\n",
        "            boundary for each misclassified sample\n",
        "        max_epochs: maximum number of passes over the\n",
        "            training set before admitting defeat\n",
        "        include_bias: whether to automatically add a\n",
        "            a constant bias feature x0\n",
        "\n",
        "    # Returns:\n",
        "        weights: vector of feature weights defining the\n",
        "            decision boundary, either same length as number of\n",
        "            columns in X or 1 greater if include_bias is True.\n",
        "            (note that a weights vector will be returned even if\n",
        "            the algorithm fails to converge)\n",
        "    \"\"\"\n",
        "    assert(X.shape[0] == len(y))\n",
        "\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "2rTVOKdyLFMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Predict binary labels from the learned boundary\n",
        "\n",
        "Implement the body of the `perceptron_predict` function in the cell below.\n",
        "\n",
        "The return predictions should be in $\\{0, 1\\}$."
      ],
      "metadata": {
        "id": "gb13HFsdB1Pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perceptron_predict ( test_X, weights ):\n",
        "    \"\"\"\n",
        "    Predict binary labels for a dataset using a specified\n",
        "    decision boundary. (This is intended for us with a boundary\n",
        "    learned with the perceptron algorithm, but any suitable\n",
        "    weights vector can be used.)\n",
        "\n",
        "    # Arguments\n",
        "        test_X: an array of sample data, where rows are samples\n",
        "            and columns are features.\n",
        "        weights: vector of feature weights defining the\n",
        "            decision boundary, either same length as number of\n",
        "            columns in X or 1 greater -- in the latter case it\n",
        "            is assumed to contain a bias term, and test_X will\n",
        "            have a constant term x0=1 prepended\n",
        "\n",
        "    # Returns\n",
        "        pred_y: a vector of predicted binary labels\n",
        "            corresponding to the samples in test_X\n",
        "\n",
        "    \"\"\"\n",
        "    assert(test_X.shape[1] in (len(weights),len(weights)-1))\n",
        "\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "SEYLZNSzLFok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 2\n",
        "\n",
        "Execute the code cell below to use your functions above to train and test a linear classifier with the perceptron algorithm. Note that this code expects to use the same linearly-separable data generated in Task 1, and will attempt to compare the margins of the boundaries discovered by the perceptron and linear SVM.\n"
      ],
      "metadata": {
        "id": "7cOF1Dyfig7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(6, 6))\n",
        "ax = fig.subplots()\n",
        "\n",
        "pw = perceptron_train(X_lin, y_lin)\n",
        "if pw is None:\n",
        "    print('perceptron not implemented')\n",
        "    utils.plot_unimplemented(ax, title='Linear Data, Perceptron')\n",
        "else:\n",
        "    utils.plot_classification_map(ax, lambda z: perceptron_predict(z, pw), X_lin, y_lin, resolution=RESOLUTION, title='Linear Data, Perceptron')\n",
        "\n",
        "    marg_ptron = geometric_margin(X_lin, y_lin, pw[1:], pw[0])\n",
        "\n",
        "    if marg_svm is None:\n",
        "        print('geometric_margin not implemented')\n",
        "    else:\n",
        "        print(f'Perceptron geometric margin: {marg_ptron:.3f}')\n",
        "        print(f'SVM margin greater by: {marg_svm - marg_ptron:.3f}')\n"
      ],
      "metadata": {
        "id": "o-zoeOPlLG15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Non-Linear Data\n",
        "\n",
        "The behaviour of linear classifiers on linearly-separable data is relatively uncontentious. It’s in the face of data that is not linearly separable that things become interesting.\n",
        "\n",
        "Notoriously, the perceptron algorithm will fail outright in such cases. A (soft margin) linear SVM will find the best boundary it can, but can still only draw a linear boundary so must concede errors somewhere. But by implicitly expanding into a much higher dimensional basis, a kernel SVM can fit nearly any kind of data."
      ],
      "metadata": {
        "id": "IhLI-HSFUjj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Generate non-linear binary data\n",
        "\n",
        "Implement the `generate_binary_nonlinear_2d` function in the code cell below.\n",
        "\n",
        "You are free to devise any generating model you like, and it is definitely worth trying out more than one candidate. Once again, feel free to ask the TAs for suggestions if you're short on inspiration.\n",
        "\n",
        "As in the previous tasks, the returned label vector should continue to use the $\\{0, 1\\}$ labelling convention.\n"
      ],
      "metadata": {
        "id": "Xxl22B-kXqRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_binary_nonlinear_2d ( num_samples, limits, rng ):\n",
        "    \"\"\"\n",
        "    Draw random samples from a binary model that is *not*\n",
        "    linearly separable in its 2D feature space. (The exact\n",
        "    form of the model is up to you.)\n",
        "\n",
        "    # Arguments\n",
        "        num_samples: number of samples to generate\n",
        "            (ie, the number of rows in the returned X\n",
        "            and the length of the returned y)\n",
        "        limits: a tuple (low, high) specifying the value\n",
        "            range of all the features x_i\n",
        "        rng: an instance of numpy.random.Generator\n",
        "            from which to draw random numbers\n",
        "\n",
        "    # Returns\n",
        "        X: a matrix of sample vectors, where\n",
        "            the samples are the rows and the\n",
        "            features are the columns\n",
        "            ie, its size should be:\n",
        "              num_samples x count\n",
        "        y: a vector of num_samples binary labels\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "    return None, None"
      ],
      "metadata": {
        "id": "yHp-QfcBX5ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 3\n",
        "\n",
        "Execute the code cell below to use your function above to generate some non-linear data, train a linear classifier on it with the perceptron algorithm and also a non-linear SVM, and plot the results of each. Do they behave as you'd expect?\n",
        "\n",
        "Once again, feel free to adjust `NUM_SAMPLES` and `RESOLUTION`, bearing in mind that high values may slow things down. You should also play around with the SVM parameters `COST` and `GAMMA` to get a feel for how these affect the learned boundary.\n"
      ],
      "metadata": {
        "id": "SG9VU7mwYtzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SAMPLES = 50\n",
        "RESOLUTION = 20\n",
        "COST = 1\n",
        "GAMMA = 0.5\n",
        "\n",
        "fig = plt.figure(figsize=(9, 5))\n",
        "axs = fig.subplots(ncols=2)\n",
        "\n",
        "# generate some data\n",
        "X_non, y_non = generate_binary_nonlinear_2d ( NUM_SAMPLES, LIMITS, shared_rng )\n",
        "\n",
        "if X_non is None:\n",
        "  print('generate_binary_nonlinear_2d not implemented')\n",
        "  utils.plot_unimplemented(axs[0], title='Non-Linear Data, Perceptron')\n",
        "  utils.plot_unimplemented(axs[1], title='Non-Linear Data, RBF SVM')\n",
        "else:\n",
        "  pw = perceptron_train(X_non, y_non)\n",
        "  if pw is None:\n",
        "    print('perceptron not implemented')\n",
        "    utils.plot_unimplemented(axs[0], title='Non-Linear Data, Perceptron')\n",
        "  else:\n",
        "    utils.plot_classification_map(axs[0], lambda z: perceptron_predict(z, pw), X_non, y_non, resolution=RESOLUTION, title=f'Non-Linear Data, Perceptron')\n",
        "\n",
        "  svm_rbf = SVC(kernel='rbf', C=COST, gamma=GAMMA)\n",
        "  svm_rbf.fit(X_non, y_non)\n",
        "\n",
        "  plot_svm_map ( axs[1], svm_rbf, X_non, y_non, RESOLUTION, title='Non-Linear Data, RBF SVM')\n"
      ],
      "metadata": {
        "id": "B9gZpHH3YuXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4: Kernel Functions\n",
        "\n",
        "The SVM implementation in `scikit-learn` (specifically [`sklearn.svm.SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)) has in-built (and optimised) support for the commonly-used **radial basis** and **polynomial** kernels, but also allows for arbitrary kernels to be used by supplying a function that computes the Gram matrix.\n",
        "\n",
        "NB: generating and using a custom kernel matrix can be very slow for large datasets, so I'd recommend keeping `NUM_SAMPLES` small for this task, especially while hacking around with it."
      ],
      "metadata": {
        "id": "swxeclj-B54s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Implement a custom kernel function\n",
        "\n",
        "Provide an implementation body for the `custom_kernel` function defined in the code cell below.\n",
        "\n",
        "Note that this function computes the kernel space inner products between two sets of data vectors. In the context of *training*, the two sets will be the same — `X1 == X2`. The resulting matrix will be square and must be symmetric and positive semi-definite. In the context of *evaluation*, the two will be different and those conditions will not hold.\n",
        "\n",
        "You can define any kernel function you like here, as long as it is valid. Can you come up with a kernel that interacts interestingly with the data model you've implemented in Task 3.1? (Spoiler alert: you may well not be able to, arbitrary kernel functions are often not very interesting.)\n",
        "\n",
        "The `gamma` parameter here is passed in from the `GAMMA` configuration variable, just as it was for the RBF kernel above. You are not required to do anything with it, but the option is there in case you want to experiment with some parameterisable kernel function.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KXLDZUltCAFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_kernel ( X1, X2, gamma=0.5 ):\n",
        "    \"\"\"\n",
        "    Custom kernel function for use with a support\n",
        "    vector classifier.\n",
        "\n",
        "    # Arguments\n",
        "        X1: first array of sample data for comparison,\n",
        "            with samples as rows and features as\n",
        "            columns (size N1 x M)\n",
        "        X2: second array of sample data for comparison,\n",
        "            with samples as rows and features as\n",
        "            columns (size N2 x M; may be the same\n",
        "            as X1)\n",
        "        gamma: a scaling hyperparameter for the similarity\n",
        "            function\n",
        "\n",
        "    # Returns\n",
        "        K: the Gram matrix for the kernel, giving the\n",
        "           kernel space inner products for each pairing of\n",
        "           a vector in X1 with one in X2 (size N1 x N2)\n",
        "    \"\"\"\n",
        "    assert(X1.shape[1] == X2.shape[1])\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "SA_hr24lNqOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 4\n",
        "\n",
        "Execute the code cell below to train an SVM using your custom kernel on data from your non-linear generator and plot the results. Does your kernel do what you think it should? Does it seem potentially useful?\n",
        "\n",
        "Once again, feel free to tweak the parameters `NUM_SAMPLES` and `RESOLUTION`, but be aware that SVMs with custom kernels can become **very** slow as the data size increases.\n"
      ],
      "metadata": {
        "id": "7DBIxl_ZitVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SAMPLES = 50\n",
        "RESOLUTION = 20\n",
        "COST = 1\n",
        "GAMMA = 0.5\n",
        "\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "ax = fig.subplots()\n",
        "\n",
        "# generate some data\n",
        "X_non, y_non = generate_binary_nonlinear_2d ( NUM_SAMPLES, LIMITS, shared_rng )\n",
        "\n",
        "if X_non is None:\n",
        "  print('generate_binary_nonlinear_2d not implemented')\n",
        "  utils.plot_unimplemented(ax, title='Non-Linear Data, Custom SVM')\n",
        "else:\n",
        "  # check custom kernel at least superficially works\n",
        "  # before trying to train an SVM with it\n",
        "  zz = np.zeros((1,1))\n",
        "  if custom_kernel(zz,zz) is None:\n",
        "    print('custom kernel not implemented')\n",
        "    utils.plot_unimplemented(ax, title='Non-Linear Data, Custom SVM')\n",
        "  else:\n",
        "    def kernel ( x, y ):\n",
        "      return custom_kernel(x, y, GAMMA)\n",
        "\n",
        "    svm_cust = SVC(kernel=kernel, C=COST)\n",
        "    svm_cust.fit(X_non, y_non)\n",
        "\n",
        "    plot_svm_map ( ax, svm_cust, X_non, y_non, RESOLUTION, title='Non-Linear Data, Custom SVM')"
      ],
      "metadata": {
        "id": "T-C7I1zBNr4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further exploration\n",
        "\n",
        "If you have exhausted the previous exercises, you might find it interesting to try out one or more of the following challenges. Doing so is entirely optional, but may provide some additional perspective that could be useful in the weeks ahead."
      ],
      "metadata": {
        "id": "ENAVpErdCTaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement a linear SVM using hinge loss optimisation\n",
        "\n",
        "As discussed in lecture 4.4, (linear) SVM fitting can be implemented via gradient descent optimisation of a regularised **hinge loss**:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{w}^{\\star}, b^{\\star} = \\text{argmin}_{\\mathbf{w},b} \\; \\frac{1}{k} \\sum_i^k \\max\\big(0, 1 - y_i (\\mathbf{x}_i \\cdot \\mathbf{w} + b)\\big) + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2\n",
        "\\end{equation}\n",
        "\n",
        "where we've chosen to average the hinge loss over the mini-batch size $k$. The hinge loss is not differentiable, but we can define a subgradient. If $L$ is the hinge loss for an individual sample:\n",
        "\n",
        "\\begin{equation}\n",
        "L(\\mathbf{x},y) = \\max\\big(0, 1-y(\\mathbf{x \\cdot w} + b)\\big)\n",
        "\\end{equation}\n",
        "\n",
        "then\n",
        "\n",
        "\\begin{align}\n",
        "\\nabla_{\\mathbf{w}} L &= -y \\, \\mathbf{x} \\cdot \\mathbb{1}\\big(y (\\mathbf{x} \\cdot \\mathbf{w} + b) < 1\\big) \\\\\n",
        "\\frac{\\partial L}{\\partial b} &= -y \\cdot \\mathbb{1}\\big(y (\\mathbf{x} \\cdot \\mathbf{w} + b) < 1\\big)\n",
        "\\end{align}\n",
        "\n",
        "and the mini-batch parameter updates are:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{w} &\\gets \\mathbf{w} - \\alpha \\bigg[ \\lambda \\mathbf{w} - \\frac{1}{k} \\sum_i y_i \\mathbf{x}_i \\cdot \\mathbb{1}\\Big(y (\\mathbf{x \\cdot w} + b) < 1\\Big) \\bigg] \\\\\n",
        "b &\\gets b - \\alpha \\bigg[ -\\frac{1}{k} \\sum_i y_i \\cdot \\mathbb{1}\\Big(y_i (\\mathbf{x}_i \\cdot \\mathbf{w} + b) < 1\\Big) \\bigg]\n",
        "\\end{align}\n",
        "\n",
        "where $\\alpha$ is the learning rate.\n",
        "\n",
        "Try implementing this and seeing how it behaves. If you consider the stochastic gradient descent case (i.e., $k=1$), how does this relate to the perceptron algorithm?\n"
      ],
      "metadata": {
        "id": "Gx96Nph4oc87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement a kernel-capable SVM using a QP solver\n",
        "\n",
        "The standard NumPy/SciPy stack does not include a quadratic programming solver, so you'll need to install other packages to perform this task. I'd suggest using [cvxpy](https://www.cvxpy.org), though you could also try [cvxopt](https://cvxopt.org).\n",
        "\n",
        "QP solvers require the problem to posed in very specific forms, which vary according to the implementation. Getting to grips with this can be a little involved, which is one reason why this task is not part of the main lab exercises. But sample code is easy to find — e.g., see [this example](https://www.cvxpy.org/examples/machine_learning/svm.html) for cvxpy, which implements a slightly different ($L_1$ regularised) SVM in QP form.\n"
      ],
      "metadata": {
        "id": "kBjB3g9-rsBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use an SVM to classify handwritten digits\n",
        "\n",
        "One of the early SVM successes, which helped build the reputation of the method, was in recognition of handwritten digits. Such recognition remains a very common computer vision testbed task.\n",
        "\n",
        "The standard dataset for digit recognition tasks is [MNIST](http://yann.lecun.com/exdb/mnist/), in which the individual digit samples are $28\\times 28$ pixel greyscale images. MNIST data can be downloaded from the [OpenML](https://openml.org) repository using `scikit-learn`'s dataset functions:\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import fetch_openml\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
        "```\n",
        "\n",
        "`scikit-learn` also provides a much smaller dataset of $8 \\times 8$ pixel handwritten digit images which you may prefer to use for more tractable computation:\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_digits\n",
        "X, y = load_digits(return_X_y=True, as_frame=False)\n",
        "```\n",
        "\n",
        "For either dataset, you will need to split the data into disjoint **train, test and validation sets** and determine appropriate hyperparameters to use for training your SVM.\n",
        "\n",
        "Note also that this is a **multiclass** problem, while SVMs are inherently binary classifiers, so an appropriate aggregation scheme is needed. However, the [`sklearn.svm.SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) class will take care of this for you, employing a **one-vs-one** decision strategy.\n"
      ],
      "metadata": {
        "id": "tnLfwPwt1Ppq"
      }
    }
  ]
}