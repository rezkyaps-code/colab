{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOpO9SKwTWbuXIKw1zs7cMG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comp0088/colab/blob/main/comp88_lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COMP0088 Lab Assignment 5\n",
        "\n"
      ],
      "metadata": {
        "id": "iZgl8O7b-fi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In this week's assignment you are asked to implement a complete **multi-layer perceptron** model, including activation and loss functions, a forward pass to process data, a backwards pass to calculate gradients, mini-batch gradient descent optimisation of the weights and finally prediction from new data. There are a lot of pieces here, but don't be intimidated — *most* of them are pretty straightforward and we'll take it step by step.\n",
        "\n",
        "For simplicity and transparency, this implementation will use just basic data structures (Python lists and dictionaries, NumPy arrays). In practice this would usually be built into a more organised class structure — we will see examples of this in week 6. If you feel particularly enthusiastic you are welcome to re-engineer these functions into something more elegant. (There are no other \"further exploration\" tasks this week — if you run out of things to do here, reward yourself with some rest and relaxation!)\n",
        "\n",
        "By default the script uses the same synthetic data as the week 3 exercises, loaded from the file `week_3_data.csv`, which is downloaded in the [Setting Up](#scrollTo=4vHvSz5pReci) section. Examples of the kinds of plots that will be produced by your finished code are shown below. Plotting functionality is provided, so your output should look similar, though by default the resolution for the class maps will be significantly lower (to avoid scaling issues).\n",
        "\n",
        "![example of completed plots](https://comp0088.github.io/assets/colab/week_5_small.jpg)\n"
      ],
      "metadata": {
        "id": "N12ZHil1_ZVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up"
      ],
      "metadata": {
        "id": "hxzyJ3xeT4LB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usual, this notebook makes use of the NumPy library for numerical computing and the Matplotlib library for plotting, so we need to import them."
      ],
      "metadata": {
        "id": "4vHvSz5pReci"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL8UJ7lgLznk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.random\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pprint\n",
        "import copy\n",
        "\n",
        "# this is probably the default, but just in case\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also as usual, fetch the shared COMP0088 lab code from the module GitHub:"
      ],
      "metadata": {
        "id": "K1RLN5QATflG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load lab code and resources\n",
        "!git clone https://github.com/comp0088/shared.git comp0088\n",
        "\n",
        "# at the moment this is all we care about\n",
        "import comp0088.utils as utils"
      ],
      "metadata": {
        "id": "v3X7LDC5KAob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up some items for later use."
      ],
      "metadata": {
        "id": "t1rjWDV14cGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# finally, set up some items for use in later code\n",
        "shared_rng = numpy.random.default_rng()\n",
        "\n",
        "LIMITS=(-5, 5)\n",
        "\n",
        "BLUE, ORANGE = plt.cm.tab10.colors[:2]\n",
        "\n",
        "DATA = pd.read_csv('comp0088/week_3_data.csv')\n"
      ],
      "metadata": {
        "id": "PfZQlfuELVwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Activation Functions\n",
        "\n",
        "The MLP for these exercises will be a simple binary classifier with two input features and a single scalar (probability) output. It will use **ReLU** (rectified linear unit) activations for the intermediate hidden layers of the network and a combination of a **sigmoid** activation on the final layer with **binary cross-entropy** for calculating the loss. You will therefore need forward and derivative implementations of all three functions.\n",
        "\n",
        "ReLU is defined as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{relu}(z) =\n",
        "\\begin{cases}\n",
        "\tz \\quad \\text{if } z > 0 \\\\\n",
        "\t0 \\quad \\text{otherwise}\n",
        "\\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "This is not differentiable at zero, but we can define a subgradient:\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{relu}^{\\prime}(z) =\n",
        "\\begin{cases}\n",
        "\t1 \\quad \\text{if } z > 0 \\\\\n",
        "\t0 \\quad \\text{otherwise}\n",
        "\\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "The sigmoid function is:\n",
        "\n",
        "\\begin{equation}\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "\\end{equation}\n",
        "\n",
        "with derivative:\n",
        "\n",
        "\\begin{equation}\n",
        "\\sigma^{\\prime}(z) = \\sigma(z)\\big(1 - \\sigma(z)\\big)\n",
        "\\end{equation}\n",
        "\n",
        "The binary cross-entropy loss is:\n",
        "\n",
        "\\begin{equation}\n",
        "L(y, \\hat{y}) = -\\big[ y \\log \\hat{y} + (1-y) \\log(1 - \\hat{y}) \\big]\n",
        "\\end{equation}\n",
        "\n",
        "with derivative:\n",
        "\n",
        "\\begin{equation}\n",
        "L^{\\prime} (y, \\hat{y})= \\frac{\\hat{y} - y}{\\hat{y} (1 - \\hat{y})}\n",
        "\\end{equation}\n",
        "\n",
        "Note that for the purposes of this implementation, all of these functions — including the loss — are to be calculated **elementwise** and should return an array of the same size and shape as the input. (For the cross-entropy functions, which have two inputs, use the shape of the `y_hat` argument.)\n",
        "\n"
      ],
      "metadata": {
        "id": "FXpzvXtJAr4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Implement forward and derivative ReLU activation\n",
        "\n",
        "Provide implementations for the `relu` (forward) and `d_relu` (derivative) functions in the cell below."
      ],
      "metadata": {
        "id": "RGJxos1yA34M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu ( z ):\n",
        "    \"\"\"\n",
        "    Rectified linear unit activation function.\n",
        "\n",
        "    # Arguments\n",
        "        z: a single number or numpy array\n",
        "\n",
        "    # Returns\n",
        "        r: a number or numpy array of the same dimensions\n",
        "            as the input value, giving the ReLU of\n",
        "            of each input value\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "    return None\n",
        "\n",
        "def d_relu ( z ):\n",
        "    \"\"\"\n",
        "    Gradient of the ReLU function\n",
        "\n",
        "    # Arguments\n",
        "        z: a single number or numpy array\n",
        "\n",
        "    # Returns\n",
        "        r: a number or numpy array of the same dimensions\n",
        "            as the input value, giving the gradient\n",
        "            of the ReLU function at each input value\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "tbnDPv-S6pHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Implement forward and derivative sigmoid activation\n",
        "\n",
        "Provide implementations for the `sigmoid` (forward) and `d_sigmoid` (derivative) functions in the cell below."
      ],
      "metadata": {
        "id": "diDAHi3gF1uE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid ( z ):\n",
        "    \"\"\"\n",
        "    Sigmoid activation function.\n",
        "\n",
        "    # Arguments\n",
        "        z: a single number or numpy array\n",
        "\n",
        "    # Returns\n",
        "        r: a number or numpy array of the same dimensions\n",
        "            as the input value, giving the sigmoid (logistic)\n",
        "            output for each input value\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "    return None\n",
        "\n",
        "def d_sigmoid ( z ):\n",
        "    \"\"\"\n",
        "    Gradient of the sigmoid function\n",
        "\n",
        "    # Arguments\n",
        "        z: a single number or numpy array\n",
        "\n",
        "    # Returns\n",
        "        r: a number or numpy array of the same dimensions\n",
        "            as the input value, giving the gradient\n",
        "            of the sigmoid function at each input value\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "ZeOh5-PzF3RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Implement forward and derivative binary cross-entropy\n",
        "\n",
        "Provide implementations for the `binary_crossentropy_loss` (forward) and `d_binary_crossentropy_loss` (derivative) functions in the cell below."
      ],
      "metadata": {
        "id": "uq7SrFmetgxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_crossentropy_loss ( y, y_hat, eps=1e-10 ):\n",
        "    \"\"\"\n",
        "    Binary cross-entropy loss for predictions, given the\n",
        "    true values.\n",
        "\n",
        "    # Arguments:\n",
        "        y: a numpy array of true binary labels.\n",
        "        y_hat: a numpy array of predicted labels,\n",
        "            as numbers in open interval (0, 1). must have\n",
        "            the same number of entries as y, but not\n",
        "            necessarily identical shape\n",
        "        eps: a small offset to avoid numerical problems\n",
        "            when predictions are very close to 0 or 1\n",
        "\n",
        "    # Returns:\n",
        "        loss: a numpy array of individual cross-entropy\n",
        "            loss values for each prediction. will be\n",
        "            the same shape as y_hat irrespective of the\n",
        "            shape of y\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "    return None\n",
        "\n",
        "def d_binary_crossentropy_loss ( y, y_hat, eps=1e-10 ):\n",
        "    \"\"\"\n",
        "    Gradient of the cross-entropy loss for predictions, given the\n",
        "    true values.\n",
        "\n",
        "    # Arguments:\n",
        "        y: a numpy array of true binary labels.\n",
        "        y_hat: a numpy array of predicted labels,\n",
        "            as numbers in open interval (0, 1). must have\n",
        "            the same number of entries as y, but not\n",
        "            necessarily identical shape\n",
        "        eps: a small offset to avoid numerical problems\n",
        "            when predictions are very close to 0 or 1\n",
        "\n",
        "    # Returns:\n",
        "        grad: a numpy array of individual cross-entropy\n",
        "            gradient values for each prediction. will be\n",
        "            the same shape as y_hat irrespective of the\n",
        "            shape of y\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "w0Sj2wCztjyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 1\n",
        "\n",
        "Execute the code cell below to plot the functions you wrote above. The function and its derivative will be plotted in the same axes. The results should look like the top row of plots from the figure in the [Introduction](#scrollTo=N12ZHil1_ZVN&line=7&uniqifier=1) section.\n"
      ],
      "metadata": {
        "id": "NPwNjtpYh_F2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(11.5, 4))\n",
        "axs = fig.subplots(ncols=3)\n",
        "\n",
        "xx = np.linspace(-5,5,100)\n",
        "yy = np.linspace(0,1,100)\n",
        "y0 = np.zeros(100)\n",
        "y1 = np.ones(100)\n",
        "\n",
        "aa = relu(xx)\n",
        "dd = d_relu(xx)\n",
        "if (aa is None) and (dd is None):\n",
        "  utils.plot_unimplemented(axs[0], 'ReLU')\n",
        "else:\n",
        "  if aa is not None: axs[0].plot(xx, aa, color=BLUE, label='Activation')\n",
        "  if dd is not None: axs[0].plot(xx, dd, color=ORANGE, linestyle='dashed', label='Gradient')\n",
        "  axs[0].set_title('ReLU')\n",
        "  axs[0].set_xlabel('$x$')\n",
        "  axs[0].set_ylabel('$f$')\n",
        "  axs[0].legend()\n",
        "\n",
        "aa = sigmoid(xx)\n",
        "dd = d_sigmoid(xx)\n",
        "if (aa is None) and (dd is None):\n",
        "  utils.plot_unimplemented(axs[1], 'Sigmoid')\n",
        "else:\n",
        "  if aa is not None: axs[1].plot(xx, aa, color=BLUE, label='Activation')\n",
        "  if dd is not None: axs[1].plot(xx, dd, color=ORANGE, linestyle='dashed', label='Gradient')\n",
        "  axs[1].set_title('Sigmoid')\n",
        "  axs[1].set_xlabel('$x$')\n",
        "  axs[1].set_ylabel('$f$')\n",
        "  axs[1].legend()\n",
        "\n",
        "ll0 = binary_crossentropy_loss(y0, yy)\n",
        "ll1 = binary_crossentropy_loss(y1, yy)\n",
        "dd0 = d_binary_crossentropy_loss(y0, yy)\n",
        "dd1 = d_binary_crossentropy_loss(y1, yy)\n",
        "\n",
        "if (ll0 is None) and (ll1 is None) and (dd0 is None) and (dd1 is None):\n",
        "  utils.plot_unimplemented(axs[2], 'Binary Cross-Entropy')\n",
        "else:\n",
        "  if ll0 is not None: axs[2].plot(yy, ll0, color=BLUE, label='Loss, $y=0$')\n",
        "  if dd0 is not None: axs[2].plot(yy, dd0, color=BLUE, linestyle='dashed', label='Gradient, $y=0$')\n",
        "  if ll1 is not None: axs[2].plot(yy, ll1, color=ORANGE, label='Loss, $y=1$')\n",
        "  if dd1 is not None: axs[2].plot(yy, dd1, color=ORANGE, linestyle='dashed', label='Gradient, $y=1$')\n",
        "  axs[2].set_title('Binary Cross-Entropy')\n",
        "  axs[2].set_xlabel('$\\\\hat{y}$')\n",
        "  axs[2].set_ylabel('$f$')\n",
        "  axs[2].set_ylim(-35,35)\n",
        "  axs[2].legend(loc='lower right')\n",
        "\n",
        "fig.tight_layout(pad=1)\n"
      ],
      "metadata": {
        "id": "m5XyX-uSFWON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Build a neural network\n",
        "\n",
        "The underlying data structures for the multi-layer perceptron are **layers**, which here will be represented by a Python dictionary, and the whole network, which will just be a Python list of layers. Each layer performs a weighted sum of its inputs followed by a non-linear activation.\n"
      ],
      "metadata": {
        "id": "YwqkbD1TzMgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Create and initialise a single network layer\n",
        "\n",
        "Implement the function `init_layer` in the code cell below.\n",
        "\n",
        "The data structure returned by this function should be a Python dictionary with the following keys and values:\n",
        "\n",
        "* `W`: a weights matrix of shape `fan_in` $\\times$ `fan_out`. We are going to be using ReLU activation for the hidden layers, so you should use **He initialisation** for the weight values, drawing from a **uniform** distribution over the range: $$\\Bigg(-\\sqrt{\\frac{6}{\\text{fan_in}}} \\, , \\; \\sqrt{\\frac{6}{\\text{fan_in}}} \\Bigg)$$\n",
        "* `b`:  a bias vector of length `fan_out`. Biases can be initialised to 0\n",
        "* `act`: the name of the specified activation function, i.e. this should store the value passed in the argument `act`\n",
        "* `shape`: a text string describing the shape mapping of the layer. This will be used to print information about a created MLP; you can format it anyway you like.\n",
        "\n",
        "Note that the dictionary keys (and all the others that will be added later) are just text strings."
      ],
      "metadata": {
        "id": "fNwmW9g_z3DC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_layer ( fan_in, fan_out, act, rng ):\n",
        "  \"\"\"\n",
        "  Create a single neural network layer.\n",
        "\n",
        "  # Arguments\n",
        "      fan_in: the number of incoming connections\n",
        "      fan_out: the number of outgoing connections\n",
        "      act: name of the activation function for the\n",
        "          layer, either \"sigmoid\" or \"relu\"\n",
        "      rng: an instance of numpy.random.Generator\n",
        "          from which to draw random numbers\n",
        "\n",
        "  # Returns\n",
        "      layer: a dict holding the layer contents, with\n",
        "          keys 'W', 'b', 'shape' and 'act'.\n",
        "          (See the description above for full details.)\n",
        "  \"\"\"\n",
        "  # TODO: implement this\n",
        "  return None"
      ],
      "metadata": {
        "id": "vGgl1jdW2WZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Create and initialise a complete network\n",
        "\n",
        "Implement the `init_mlp` function in the code cell below to build a list of individual layers to match a simple specification. The layers themselves are just dictionaries returned by `init_layer`.\n",
        "\n",
        "Iterate over the list of tuples in `spec` and create a layer for each one except the last, setting the `fan_out` of each layer as the `fan_in` of the next. (Use the activation from the input tuple; the last one will have an activation of `None`.)\n"
      ],
      "metadata": {
        "id": "PPeC3Ido220e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_mlp ( spec, rng ):\n",
        "  \"\"\"\n",
        "  Build a neural network according to the\n",
        "  given specification.\n",
        "\n",
        "  # Arguments\n",
        "      spec: an iterable of tuples (fan_in, act)\n",
        "          specifying the configuration of the network layers.\n",
        "          there must be at least 2 elements; the last is only\n",
        "          used to determine output size of the layer before,\n",
        "          it does not create a layer of its own\n",
        "      rng: an instance of numpy.random.Generator\n",
        "          from which to draw random numbers\n",
        "\n",
        "  # Returns\n",
        "      mlp: a list of layer dicts\n",
        "  \"\"\"\n",
        "  assert(len(spec) > 1)\n",
        "\n",
        "  # TODO: implement this\n",
        "  return None"
      ],
      "metadata": {
        "id": "SgiJSmFw3D1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 2\n",
        "\n",
        "Run the cell below to perform a simple test of your functions above. Note that this is pretty superficial as the code isn't doing very much yet."
      ],
      "metadata": {
        "id": "AGzlHPZJuWFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn = init_mlp( ((2, 'relu'), (50, 'relu'), (50, 'sigmoid'), (1, None)), shared_rng )\n",
        "assert(len(nn)==3)\n",
        "assert('shape' in nn[0])\n",
        "assert('act' in nn[0])\n",
        "assert('W' in nn[0])\n",
        "assert('b' in nn[0])\n",
        "pprint.pprint( [ (layer['shape'], [layer['W'].shape, layer['b'].shape], layer['act']) for layer in nn] )\n",
        "print(f'\\nMean of middle layer weights (should be close to zero): {np.mean(nn[1][\"W\"]):.3f}')"
      ],
      "metadata": {
        "id": "ZOil92uuuwC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Perform a forward pass\n",
        "\n",
        "In the forward pass through the network, layers receive input data and produce outputs, storing information they will need to calculate gradients. The outputs from each layer become the inputs to the next.\n"
      ],
      "metadata": {
        "id": "r_cF8l272Pqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Perform a forward pass through a single network layer\n",
        "\n",
        "Implement the `layer_forward` function in the code cell below to perform the forward pass through one layer.\n",
        "\n",
        "The forward pass computes\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{Z} &= \\mathbf{XW} + \\mathbf{b} \\\\\n",
        "\\mathbf{A} &= \\text{activation}(\\mathbf{Z})\n",
        "\\end{align}\n",
        "\n",
        "**IMPORTANT**: You will need the input data, linear combination and output for the backwards pass, so **store these in the `layer` dictionary** with the new keys `X`, `Z` and `A`, respectively.\n"
      ],
      "metadata": {
        "id": "Vf2BdwcK2XWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_forward ( layer, X ):\n",
        "    \"\"\"\n",
        "    Run a forward pass of data through a layer, storing\n",
        "    intermediate values in the layer dict.\n",
        "\n",
        "    # Arguments\n",
        "        layer: a layer dict as created by init_layer\n",
        "        X: the input data to the layer, a matrix\n",
        "            where the columns are features and the\n",
        "            rows are samples. feature count must\n",
        "            match the layer's fan_in\n",
        "\n",
        "    # Returns\n",
        "        A: the layer's output activations, a matrix where the\n",
        "            columns are (fan_out) features and the rows are\n",
        "            samples\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "wlI7eYlm2jmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Perform a forward pass through a whole network\n",
        "\n",
        "Implement the `mlp_forward` function in the code cell below to perform a forward pass through the whole network. This should just iterate over the layers in turn, calling `layer_forward` for each one. The input to the first layer is the network input data argument `X`, while the input for subsequent layers is the output from the layer before."
      ],
      "metadata": {
        "id": "eyzGXkfy2lFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp_forward ( mlp, X ):\n",
        "    \"\"\"\n",
        "    Run a forward pass through a whole neural net.\n",
        "\n",
        "    # Arguments\n",
        "        mlp: a list of layer dicts, as created by init_mlp\n",
        "        X: the input data to the network, a matrix\n",
        "            where the columns are features and the\n",
        "            rows are samples. feature count must\n",
        "            match the first layer's fan_in\n",
        "\n",
        "    # Returns\n",
        "        A: the output activations of the final network layer\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "bkGhOnME2yHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 3\n",
        "\n",
        "Run the cell below to perform a simple test of your forward pass processing. As with Task 2, the checks here are pretty rudimentary — the functions will be more thoroughly exercised later on."
      ],
      "metadata": {
        "id": "f08vkyG52y-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build a simple (non-random) network and some test data\n",
        "# we'll reuse this later, so make it a function\n",
        "def make_test ():\n",
        "  nn = [{'W': np.array([[ 1.,0.], [0.,-1.]]),\n",
        "    'b': np.array([1., 0.]),\n",
        "    'shape': '1->2',\n",
        "    'act': 'relu'},\n",
        "  {'W': np.array([[ 1.], [-1.]]),\n",
        "    'b': np.array([1.]),\n",
        "    'shape': '2->1',\n",
        "    'act': 'sigmoid'}]\n",
        "  X = np.eye(2)\n",
        "\n",
        "  return nn, X\n",
        "\n",
        "nn1, X1 = make_test()\n",
        "A1 = mlp_forward(nn1, X1)\n",
        "\n",
        "# look at the updated version\n",
        "display(nn1)\n",
        "\n",
        "# check some keys and values are what we expect\n",
        "assert('X' in nn1[0])\n",
        "assert(np.allclose(X1, nn1[0]['X']))\n",
        "\n",
        "assert('A' in nn1[0])\n",
        "assert(np.allclose(nn1[0]['A'], np.array([[2., 0.], [1., 0.]])))\n",
        "\n",
        "assert('X' in nn1[0])\n",
        "assert(np.allclose(nn1[0]['A'], nn1[1]['X']))\n",
        "\n",
        "assert('Z' in nn1[1])\n",
        "assert(np.allclose(nn1[1]['Z'], np.array([[3], [2]])))\n",
        "\n",
        "assert(np.allclose(A1, np.array([[0.95257413], [0.88079708]])))"
      ],
      "metadata": {
        "id": "FRL8tCW525Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4: Perform a backward pass\n",
        "\n",
        "In the backward pass through the network, layers receive the downstream loss gradient (with respect to their output activations) and calculate their loss gradients with respect to their weights, bias and inputs, storing these for subsequent learning. The gradients with respect to the inputs from each layer are then propagated back as the downstream gradients for the previous layer."
      ],
      "metadata": {
        "id": "ZqnAwF6k26AR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Perform a backward pass through a single network layer\n",
        "\n",
        "Implement the `layer_backward` function in the code cell below to perform a backward pass through a single layer.\n",
        "\n",
        "The gradients propagate backwards by multiplication. For the activations, the derivative evaluation and multiplication both occur **elementwise**:\n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla_{\\mathbf{Z}} = \\nabla_{\\mathbf{A}} \\times \\text{activation}^{\\prime}(\\mathbf{Z})\n",
        "\\end{equation}\n",
        "\n",
        "For the weights and inputs, the calculation involves inner products, so we can express it using matrix multiplication, taking care that the right product winds up in the right place in the matrix. The relevant expressions are:\n",
        "\n",
        "\\begin{align}\n",
        "\\nabla_{\\mathbf{X}} &= \\nabla_{\\mathbf{Z}} \\mathbf{W}^{\\mathsf{T}} \\\\\n",
        "\\nabla_{\\mathbf{W}} &= \\mathbf{X}^{\\mathsf{T}} \\nabla_{\\mathbf{Z}}\n",
        "\\end{align}\n",
        "\n",
        "For the bias term, there is no input dependence, and so the gradient vector is $\\mathbf{1} \\cdot \\nabla_{\\mathbf{Z}}$ (i.e, the columnwise sum of $\\nabla_{\\mathbf{Z}}$).\n",
        "\n",
        "Once again, **save all the gradients in the layer dictionary** with keys `dA`, `dZ`, `dW`, `db` and `dX`, returning the latter.\n",
        "\n",
        "**NB**: while this function can be implemented in just a few lines of code, there's a lot going on here, so it's worth taking some time to understand it. This is the core of the backpropagation process that makes it possible to train a neural networks.\n"
      ],
      "metadata": {
        "id": "s9nesQU03BHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_backward ( layer, dA ):\n",
        "    \"\"\"\n",
        "    Run a backward pass of gradients through a layer, storing\n",
        "    computed values in the layer dict. The forward pass must\n",
        "    have been performed first.\n",
        "\n",
        "    # Arguments\n",
        "        layer: a layer dict as created by init_layer\n",
        "        dA: the gradients of the loss with respect to the\n",
        "            forward pass activations. a matrix the same shape\n",
        "            as those previously computed activations.\n",
        "\n",
        "    # Returns\n",
        "        dX: the gradients of the loss with respect to the\n",
        "            layer inputs from the forward pass\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "yn07x1ZE3TWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Perform a backward pass through a whole network\n",
        "\n",
        "Implement the `mlp_backward` function in the code cell below to perform a backward pass through the complete network.\n",
        "\n",
        "Like the forward pass, the backward pass simply involves iterating through the network calling `layer_backward` on each layer. Remember to traverse the list in reverse, and pass the gradient returned by each layer to the one before it."
      ],
      "metadata": {
        "id": "OsI7aXsC3UO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp_backward ( mlp, d_loss ):\n",
        "    \"\"\"\n",
        "    Backpropagate gradients through the whole neural net.\n",
        "    The forward pass must have been performed first.\n",
        "\n",
        "    # Arguments\n",
        "        mlp: a list of layer dicts, as created by init_mlp\n",
        "        d_loss: the gradients of the loss at the final\n",
        "            layer output, a matrix the same shape\n",
        "            as previously computed activations.\n",
        "\n",
        "    # Returns\n",
        "        None\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "ud_02VhF3ae-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 4\n",
        "\n",
        "Run the cell below to perform a simple test of your backward pass processing. Once again, this is not an exhaustive test, but it should present a reasonable first hurdle.\n"
      ],
      "metadata": {
        "id": "fGFt3eFU3fz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remake test network each time since the passes modify it\n",
        "nn1, X1 = make_test()\n",
        "A1 = mlp_forward(nn1, X1)\n",
        "\n",
        "# run backward pass with final gradients all set to 1\n",
        "dA1 = np.ones_like(A1)\n",
        "mlp_backward(nn1, dA1)\n",
        "\n",
        "# print the updated network\n",
        "display(nn1)\n",
        "\n",
        "# check some keys and values are what we expect\n",
        "assert('dA' in nn1[1])\n",
        "assert(np.allclose(dA1, nn1[1]['dA']))\n",
        "\n",
        "assert('dZ' in nn1[1])\n",
        "assert(np.allclose(nn1[1]['dZ'], np.array([[0.04517666], [0.10499359]])))\n",
        "\n",
        "assert('dW' in nn1[1])\n",
        "assert(np.allclose(nn1[1]['dW'], np.array([[0.1953469], [0.]])))\n",
        "\n",
        "assert('dX' in nn1[1])\n",
        "assert('dA' in nn1[0])\n",
        "assert(np.allclose(nn1[0]['dA'], nn1[1]['dX']))\n",
        "\n"
      ],
      "metadata": {
        "id": "k3_hlQzA3imX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5: Update the network weights\n",
        "\n",
        "We'll train the network using vanilla gradient descent, simply adjusting the weights and bias at each iteration by a fraction of the gradient:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{W} &\\gets \\mathbf{W} - \\alpha \\nabla_{\\mathbf{W}} \\\\\n",
        "\\mathbf{b} &\\gets \\mathbf{b} - \\alpha \\nabla_{\\mathbf{b}}\n",
        "\\end{align}\n",
        "\n",
        "where $\\alpha$ is the learning rate.\n"
      ],
      "metadata": {
        "id": "mEWkxpfE3jPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Update the weights for a single network layer\n",
        "\n",
        "Implement the `layer_update` function in the cell below to update the weights and bias of a single layer by a fraction (`lr`) of the gradients calculated during the backward pass.\n",
        "\n",
        "Remember to update the values assigned to the keys `W` and `b` in the layer dictionary."
      ],
      "metadata": {
        "id": "A2rR99bZ3tR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_update ( layer, lr ):\n",
        "    \"\"\"\n",
        "    Update layer weights & biases according to the previously\n",
        "    computed gradients. Forward and backward passes\n",
        "    must both have been performed.\n",
        "\n",
        "    # Arguments\n",
        "        layer: a layer dict as created by init_layer\n",
        "        lr: the learning rate\n",
        "\n",
        "    # Returns\n",
        "        None\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "ybSjOyZ040sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Update the weights for a whole network\n",
        "\n",
        "Implement the `mlp_update` function below to update all the weights and biases in a network. Once again, this just involves iterating over the layers and calling `layer_update` on each.\n",
        "\n",
        "In what order should this take place?"
      ],
      "metadata": {
        "id": "lOnb2Rqy41O6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp_update ( mlp, lr ):\n",
        "    \"\"\"\n",
        "    Update all network weights & biases according to the\n",
        "    previously computed gradients. Forward and backward passes\n",
        "    must both have been performed.\n",
        "\n",
        "    # Arguments\n",
        "        mlp: a list of layer dicts, as created by init_mlp\n",
        "        lr: the learning rate\n",
        "\n",
        "    # Returns\n",
        "        None\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "    pass"
      ],
      "metadata": {
        "id": "kbohABFB47du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 5"
      ],
      "metadata": {
        "id": "Df9RRvGS476J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remake & rerun test network each time since the passes modify it\n",
        "nn1, X1 = make_test()\n",
        "A1 = mlp_forward(nn1, X1)\n",
        "dA1 = np.ones_like(A1)\n",
        "mlp_backward(nn1, dA1)\n",
        "\n",
        "# duplicate before updating so we can compare before & after\n",
        "nn2 = copy.deepcopy(nn1)\n",
        "\n",
        "LR = 0.1\n",
        "mlp_update( nn2, LR )\n",
        "display(nn2)\n",
        "\n",
        "# check params have been updated as expected\n",
        "assert(np.allclose(nn2[0]['W'], nn1[0]['W'] - LR * nn1[0]['dW']))\n",
        "assert(np.allclose(nn2[0]['b'], nn1[0]['b'] - LR * nn1[0]['db']))\n",
        "assert(np.allclose(nn2[1]['W'], nn1[1]['W'] - LR * nn1[1]['dW']))\n",
        "assert(np.allclose(nn2[1]['b'], nn1[1]['b'] - LR * nn1[1]['db']))"
      ],
      "metadata": {
        "id": "QiXduuxp5AnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 6: Train the network\n",
        "\n",
        "We're going to train the network using **mini-batch gradient descent**. This means iterating over the training data in bite size chunks, and for each chunk executing a forward pass, then a backward pass, then a weights update.\n",
        "\n",
        "One complete run through all the mini-batches in the training set is known as an **epoch**.\n"
      ],
      "metadata": {
        "id": "3vyiOEHy5BQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 Train the network on a single mini-batch of data\n",
        "\n",
        "Implement the `mlp_minibatch` function in the cell below to do a single training iteration of a network with a single mini-batch of data.\n",
        "\n",
        "The data passed in `X` and `y` is just the single chunk, so you can pass it straight to the network.\n",
        "\n",
        "The output of `mlp_forward` is from the final layer sigmoid, representing the probabilistic predictions $\\hat{\\mathbf{y}}$. You will need to use this together with the ground truth labels `y` to calculate the cross entropy loss and also the loss gradient. The latter should then be propagated back through the network, and the former should be returned. (Divide by the number of samples to get the mean loss.)"
      ],
      "metadata": {
        "id": "GCpoKt_s6wnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp_minibatch ( mlp, X, y, lr ):\n",
        "    \"\"\"\n",
        "    Fit a neural network to a single mini-batch\n",
        "    of training data.\n",
        "\n",
        "    # Arguments\n",
        "        mlp: a list of layer dicts, as created by init_mlp\n",
        "        X: an array of sample data, where rows are samples\n",
        "            and columns are features. feature dimension must\n",
        "            match the input dimension of mlp.\n",
        "        y: vector of binary class labels corresponding to the\n",
        "            samples, must be same length as number of rows in X\n",
        "        lr: the learning rate\n",
        "\n",
        "    # Returns\n",
        "        loss: the mean training loss over the minibatch\n",
        "    \"\"\"\n",
        "    assert(X.shape[0] == len(y))\n",
        "    assert(X.shape[-1] == mlp[0]['W'].shape[0])\n",
        "\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "WVjC-llY6teq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 Train the network for one complete epoch\n",
        "\n",
        "Implement `mlp_epoch` in the code cell below to train the network on all the provided data, one mini-batch at a time.\n",
        "\n",
        "This is basically an exercise in data management, splitting the data and labels into matched chunks to pass to `mlp_minibatch`. You may find the [`permutation`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.permutation.html) function useful to randomise the order of samples each epoch.\n",
        "\n"
      ],
      "metadata": {
        "id": "NQCc1epQ6zBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp_epoch ( mlp, X, y, batch, lr, rng ):\n",
        "    \"\"\"\n",
        "    Fit a neural network for one epoch -- ie, a single\n",
        "    pass through the data in minibatches of specified size.\n",
        "\n",
        "    # Arguments\n",
        "        mlp: a list of layer dicts, as created by init_mlp\n",
        "        X: an array of sample data, where rows are samples\n",
        "            and columns are features. feature dimension must\n",
        "            match the input dimension of mlp.\n",
        "        y: vector of binary class labels corresponding to the\n",
        "            samples, must be same length as number of rows in X\n",
        "        batch: the size of minibatches to train\n",
        "        lr: the learning rate\n",
        "        rng: an instance of numpy.random.Generator\n",
        "            from which to draw random numbers\n",
        "\n",
        "    # Returns\n",
        "        loss: the mean training loss over the whole dataset\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "e8TZ5HkJ60Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3 Train the network for multiple epochs\n",
        "\n",
        "Implement `mlp_train` in the code cell below to train a network for multiple epochs. This should just call `mlp_epoch` the requested number of times, recording the loss history as it goes."
      ],
      "metadata": {
        "id": "70WxmpJf63ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp_train ( mlp, X, y, batch, epochs, lr, rng ):\n",
        "    \"\"\"\n",
        "    Fit a neural network iteratively for multiple\n",
        "    epochs.\n",
        "\n",
        "    # Arguments\n",
        "        mlp: a list of layer dicts, as created by init_mlp\n",
        "        X: an array of sample data, where rows are samples\n",
        "            and columns are features. feature dimension must\n",
        "            match the input dimension of mlp.\n",
        "        y: vector of binary class labels corresponding to the\n",
        "            samples, must be same length as number of rows in X\n",
        "        batch: the size of minibatches to train\n",
        "        epochs: number of epochs to train\n",
        "        lr: the learning rate\n",
        "        rng: an instance of numpy.random.Generator\n",
        "            from which to draw random numbers\n",
        "\n",
        "    # Returns\n",
        "        loss: a list of the mean training loss at each epoch\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "nBMqA8RU65Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Note that there isn't a \"Run Task 6\" cell here — we defer this to the next section.)"
      ],
      "metadata": {
        "id": "9qV_0WrlaBCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 7: Test the network\n",
        "\n",
        "The `mlp_forward` function already allows you to run test data through a network, but it is useful also to be able to generate class labels rather than probabilities.\n"
      ],
      "metadata": {
        "id": "O0fh_kU25IyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1 Make predictions\n",
        "\n",
        "Implement the `mlp_predict` function to make predictions on binary class from a neural net. (This will be used in the testing section below to plot a classification map.)"
      ],
      "metadata": {
        "id": "5i2vldp-6-D3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp_predict ( mlp, X, thresh=0.5 ):\n",
        "    \"\"\"\n",
        "    Make class predictions from a neural network.\n",
        "\n",
        "    # Arguments\n",
        "        mlp: a list of layer dicts, as created by init_mlp\n",
        "        X: an array of test data, where rows are samples\n",
        "            and columns are features. feature dimension must\n",
        "            match the input dimension of mlp.\n",
        "        thresh: the decision threshold\n",
        "\n",
        "    # Returns\n",
        "        y_hat: a vector of predicted binary labels for X\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "E7Flj4ny6_d1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2 Train and test your classifier\n",
        "\n",
        "Run the cell below to use your functions above to build a neural net, train it on our loaded data, test it and plot the results.\n",
        "\n",
        "There are some configuration variables at the top of the cell which may significantly affect how the neural net behaves (and also how long this all takes to run). Try playing around with these and seeing what happens."
      ],
      "metadata": {
        "id": "qLYxwLD8_BO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# configure\n",
        "\n",
        "# model structure\n",
        "# input and output sizes are predefined for this task,\n",
        "# so here we just specify the number of neurons in each\n",
        "# hidden layer\n",
        "HIDDEN = (4, 3)\n",
        "\n",
        "# training hyperparameters\n",
        "BATCH = 1\n",
        "EPOCHS = 100\n",
        "LR = 1e-2\n",
        "\n",
        "# number of samples in training set\n",
        "NUM_SAMPLES = 50\n",
        "\n",
        "# plotting resolution for the classification map\n",
        "RESOLUTION = 20\n",
        "\n",
        "# --------\n",
        "\n",
        "# select training data\n",
        "X = DATA[['X1','X2']].values[:NUM_SAMPLES,:]\n",
        "y = DATA['Binary'].values[:NUM_SAMPLES]\n",
        "\n",
        "fig = plt.figure(figsize=(11.5, 4))\n",
        "axs = fig.subplots(ncols=3)\n",
        "\n",
        "# build model\n",
        "layers = ((2, 'relu'),) + tuple((n, 'relu') for n in HIDDEN[:-1]) + ((HIDDEN[-1], 'sigmoid'), (1, None))\n",
        "nn = init_mlp(layers, shared_rng)\n",
        "\n",
        "if nn is None:\n",
        "  utils.plot_unimplemented(axs[0], 'Training Loss')\n",
        "  utils.plot_unimplemented(axs[1], f'{len(layers)-1}-Layer MLP')\n",
        "  utils.plot_unimplemented(axs[2], 'Output Activations')\n",
        "else:\n",
        "  # train it\n",
        "  loss = mlp_train(nn, X, y, BATCH, EPOCHS, LR, shared_rng)\n",
        "  if loss is None:\n",
        "    utils.plot_unimplemented(axs[0], 'Training Loss')\n",
        "    utils.plot_unimplemented(axs[1], f'{len(layers)-1}-Layer MLP')\n",
        "    utils.plot_unimplemented(axs[2], 'Output Activations')\n",
        "  else:\n",
        "    axs[0].plot(loss)\n",
        "    axs[0].set_title('Training Loss')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_ylabel('Mean Binary Cross-Entropy')\n",
        "\n",
        "    utils.plot_classification_map(axs[1], lambda z: mlp_predict(nn, z), X, y,\n",
        "                                  resolution=RESOLUTION, limits=LIMITS,\n",
        "                                  title=f'{len(layers)-1}-Layer MLP')\n",
        "\n",
        "    # everything from the last forward pass is stashed in the network\n",
        "    # so we can pull out the activations from the last layer to plot\n",
        "    im = nn[-1]['A'].reshape((RESOLUTION,RESOLUTION))\n",
        "    axs[2].imshow(im.T, origin='lower', extent=LIMITS * 2, cmap='Greens')\n",
        "    axs[2].set_title('Output Activations')\n",
        "    axs[2].set_xlabel('$x_1$')\n",
        "    axs[2].set_ylabel('$x_2$')\n",
        "\n",
        "fig.tight_layout(pad=1)"
      ],
      "metadata": {
        "id": "vXObCTmA_Dre"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}